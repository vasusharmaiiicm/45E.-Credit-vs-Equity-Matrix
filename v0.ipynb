{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e670e7-6fdc-4f86-888b-69ffbe02ca11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "from xbbg import blp\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, Dropdown, HBox, VBox, Button, Output, Text, widgets\n",
    "import sympy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, update_display\n",
    "from IPython import get_ipython\n",
    "# import backtrader.plot as btplot\n",
    "import matplotlib.dates as mdates\n",
    "from pydataquery import DataQuery\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import yfinance as yf\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##### Style df\n",
    "def bold_zscore(val):\n",
    "    return 'font-weight: bold' if val else ''\n",
    "\n",
    "def add_black_line(row):\n",
    "    return ['border-bottom: 3px solid black' if row.name in [2, 7, 13, 15, 17, 21] else '' for _ in row]\n",
    "\n",
    "def color_negative_red_positive_green_basis(col: pd.Series):\n",
    "    if col.empty:\n",
    "        return ['' for _ in col]\n",
    "\n",
    "    def safe_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    float_col = col.apply(safe_float)\n",
    "    min_val = float_col.min(skipna=True)\n",
    "    max_val = float_col.max(skipna=True)\n",
    "\n",
    "    def value_to_color(val):\n",
    "        if val is None:\n",
    "            return ''\n",
    "        if val < 0 and min_val < 0:\n",
    "            frac = val / min_val\n",
    "            frac = max(min(frac, 1), 0)\n",
    "            r = int(255 - (255 - 87) * frac)\n",
    "            g = int(255 - (255 - 187) * frac)\n",
    "            b = int(255 - (255 - 138) * frac)\n",
    "            return f'background-color: rgba({r},{g},{b},0.75)'\n",
    "        elif val > 0 and max_val > 0:\n",
    "            frac = val / max_val\n",
    "            frac = max(min(frac, 1), 0)\n",
    "            r = int(255 - (255 - 230) * frac)\n",
    "            g = int(255 - (255 - 135) * frac)\n",
    "            b = int(255 - (255 - 115) * frac)\n",
    "            return f'background-color: rgba({r},{g},{b},0.75)'\n",
    "        return ''\n",
    "\n",
    "    return [value_to_color(v) for v in float_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69dd64f-a300-4246-af64-5173def82c92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ################################## Updating betas of all pairs\n",
    "# all_start_date = '2020-01-01'#str((datetime.now()-timedelta(days=365*2.5)).date())\n",
    "\n",
    "# labels = {\n",
    "#         \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         # \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "#         \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         # \"CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dfx = df.copy()\n",
    "# dfx.index = pd.to_datetime(dfx.index)\n",
    "\n",
    "# end_date = dfx.index[-1]\n",
    "# df_pairs = dfx.copy()\n",
    "# dfx = dfx.resample('W').last()\n",
    "# master_dfx = (dfx.diff(1))/dfx.shift(1)\n",
    "\n",
    "# start_date = all_start_date\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity','IEAC LN Equity','IHYG LN EQUITY','BKLN US EQUITY']\n",
    "# securities = ['HYG US Equity','EMB US Equity','VCIT US Equity',]\n",
    "# fields1 = ['YAS_MOD_DUR']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = df.copy()\n",
    "\n",
    "# securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','VCIT US Equity',]\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['QW3I INDEX']\n",
    "# fields = ['MODIFIED_DURATION']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPX INDEX', 'RSP US EQUITY', 'RTY INDEX']\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# df1.sort_index(inplace=True)\n",
    "# all_dq_initial_data = df1.copy()\n",
    "\n",
    "# ############################################### bbg_data\n",
    "# bbg_tick = ['HYG US Equity','EMB US Equity','VCIT US Equity',\\\n",
    "#             'LT03TRUU INDEX','LT09TRUU INDEX',\n",
    "#            'QW3I INDEX']\n",
    "\n",
    "# fields1 = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers = bbg_tick, start_date = start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = ['TR ' + list(item)[0].split(' ')[0] for item in list(df.columns)]\n",
    "# all_bbg_data_initial = df.copy()\n",
    "\n",
    "# ################################################################\n",
    "\n",
    "# df_beta = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "# df_beta['Date'] = pd.to_datetime(df_beta['Date'])\n",
    "# df_beta = df_beta.set_index('Date')\n",
    "# df_beta = df_beta[df_beta.index<=datetime.now()]\n",
    "\n",
    "# all_beta_df = None\n",
    "# backtest_last_date = df_beta.index[-1].date() - timedelta(days=8)\n",
    "\n",
    "# while backtest_last_date <= datetime.now().date():\n",
    "#     try:\n",
    "#         dfx = master_dfx.copy()\n",
    "#         # all_start_date = str((datetime.now()-timedelta(days=365*2.5)).date())\n",
    "        \n",
    "#         tickers = {\n",
    "#             tuple(['ER CDX HY 5Y','TR HYG - (HYG DUR/LT03TRUU DUR) * TR LT03TRUU']): [1,1],\n",
    "#             tuple(['ER CDX EM 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX HY 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 10Y','TR LQD - (LQD DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             tuple(['ER CDX IG 5Y','TR VCIT - (VCIT DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 10Y','TR VCIT - (VCIT DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "            \n",
    "#             # tuple(['ER ITRX MAIN 5Y','TR IEAC - (IEAC DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#             # tuple(['ER ITRX XOVER 5Y','TR IHYG - (IHYG DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#             # tuple(['ER CDX HY 5Y','TR IHYG - (IHYG DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#         }\n",
    "        \n",
    "#         ratios = list(tickers.values())\n",
    "#         ratios = [list(item) for item in ratios]\n",
    "        \n",
    "#         tick = list(tickers.keys())\n",
    "#         tick = [list(item) for item in tick]\n",
    "        \n",
    "#         bbg_data_initial = all_bbg_data_initial[all_bbg_data_initial.index<=pd.to_datetime(backtest_last_date).date()].copy()\n",
    "#         dq_initial_data = all_dq_initial_data[all_dq_initial_data.index<=pd.to_datetime(backtest_last_date).date()].copy()\n",
    "    \n",
    "#         df2 = dq_initial_data.copy()\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "        \n",
    "#         dfA = df2[[col for col in df2.columns if (col.startswith(\"TR\") or col.startswith(\"ER\"))]]\n",
    "#         dfA = dfA.resample('W').last()\n",
    "#         dfA = dfA.diff(1)/dfA.shift(1)\n",
    "#         dfB = df2[[col for col in df2.columns if col.endswith(\"DUR\")]]\n",
    "#         df_dur = dfB.copy()\n",
    "#         dfB = dfB.resample('W').mean()\n",
    "#         df2 = pd.concat([dfA, dfB], axis=1)\n",
    "        \n",
    "#         df3 = df2.copy()\n",
    "#         for i in range(len(tickers)):\n",
    "#             item = list(list(tickers.keys())[i])[1]\n",
    "#             if ' - ' in item:\n",
    "#                 first = item.split(' - ')[0]\n",
    "#                 second = item.split(' - ')[1].split('/')[0][1:]\n",
    "#                 third = item.split(' - ')[1].split('/')[1].split(\")\")[0]\n",
    "#                 fourth = item.split(' - ')[1].split('/')[1].split(\"* \")[1]\n",
    "#                 df3[item] = df3[first] - (df3[second]/df3[third]) * df3[fourth]\n",
    "#         df3 = df3[[col for col in df3.columns if ' - ' in col]]\n",
    "        \n",
    "#         dfx.index = pd.to_datetime(dfx.index)\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "#         df3.index = pd.to_datetime(df3.index)\n",
    "#         df3 = pd.concat([dfx,df2,df3],axis=1)\n",
    "        \n",
    "#         df3 = df3.iloc[:,~df3.columns.duplicated()]\n",
    "        \n",
    "#         df4 = df3.copy()\n",
    "#         df_deming = df4.copy()\n",
    "        \n",
    "#         # df4 = df4[df4.index.year != 2020]\n",
    "#         # df4 = df4[df4.index > pd.to_datetime('2021-01-01')]\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             df5 = df4[tick[i]].copy()\n",
    "#             for j in range(len(df5)-24):\n",
    "#                 Y = df5[tick[i]].iloc[j:j+24].dropna()[tick[i][0]] * 1#ratios[i][0]\n",
    "#                 X = df5[tick[i]].iloc[j:j+24].dropna()[tick[i][1]] * 1#ratios[i][1]\n",
    "#                 if ((len(Y) > 20) and (len(X) == len(Y))):\n",
    "#                     X = sm.add_constant(X)\n",
    "#                     model = sm.OLS(Y,X).fit()\n",
    "#                     df4.loc[f'{df5.index[j+24]}',f'{tick[i][0]} vs. {tick[i][1]}'] = model.params.iloc[1]\n",
    "        \n",
    "#         df5 = df4[[col for col in df4.columns if 'vs.' in col]].copy()\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "        \n",
    "#         df5 = df5.dropna(how='all')\n",
    "        \n",
    "#         df6 = df5.rolling(window=104).mean().iloc[[-1]].copy() #52 weeks in a year => 104 weeks in 2 years\n",
    "        \n",
    "#         df6 = df6[[col for col in df6.columns if ' - ' in col]]\n",
    "        \n",
    "#         l1 = [item.split(\" vs. \")[0] for item in df6.columns]\n",
    "#         l2 = [item.split(\" vs. \")[1].split(' - ',1)[0] for item in df6.columns]\n",
    "#         l3 = [item.split(\" vs. \")[1].split(' - ',1)[1].split(\" * \")[1] for item in df6.columns]\n",
    "#         l = []\n",
    "        \n",
    "#         for i in range(len(l1)):\n",
    "#             l += [tuple([l1[i]] + [l2[i]] + [l3[i]])]\n",
    "        \n",
    "#         m = list(df6.iloc[-1])\n",
    "#         m = [[1,-1*item] for item in m]\n",
    "        \n",
    "#         tickers_2 = dict(zip(l,m))\n",
    "        \n",
    "#         ratios = list(tickers_2.values())\n",
    "#         ratios = [list(item) for item in ratios]\n",
    "        \n",
    "#         tick = list(tickers_2.keys())\n",
    "#         tick = [list(item) for item in tick]\n",
    "        \n",
    "        \n",
    "#         df = bbg_data_initial.copy()\n",
    "#         df.index = pd.to_datetime(df.index)\n",
    "#         df = df.resample('W').last()\n",
    "#         df = df.diff()/df.shift(1)\n",
    "#         df2 = pd.concat([dfx,df],axis=1)\n",
    "#         df2 = df2.dropna()\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             df2[f'{ratios[i][0]}*{tick[i][0]} + ({ratios[i][1]})*{tick[i][1]}'] = (ratios[i][0] * df2[tick[i][0]] + ratios[i][1] * df2[tick[i][1]])\n",
    "        \n",
    "#         df4 = df2.copy()\n",
    "#         df4.index = pd.to_datetime(df4.index)\n",
    "        \n",
    "#         # df4 = df4[df4.index > pd.to_datetime('2021-01-01')]\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             for col in df4.columns:\n",
    "#                 if ((tick[i][0] in col) and (tick[i][1] in col)):\n",
    "#                     globals()[f'df_{i}'] = df4[[col,tick[i][2]]].dropna().copy()\n",
    "#                     val = []\n",
    "#                     for z in range(len(globals()[f'df_{i}'])-24):\n",
    "#                         Y = globals()[f'df_{i}'].iloc[z:z+24,0]\n",
    "#                         X = globals()[f'df_{i}'].iloc[z:z+24,1]\n",
    "#                         X = sm.add_constant(X)\n",
    "#                         model = sm.OLS(Y,X).fit()\n",
    "#                         val.append(model.params.iloc[1])\n",
    "#                     val1 = [np.nan] * 24 + val\n",
    "#                     globals()[f'df_{i}'][f'{list(globals()[f'df_{i}'].columns)}'] = val1\n",
    "        \n",
    "        \n",
    "        \n",
    "#         periods = {'6M':0.5, '1Y':1, '2Y':2, '3Y':3, '4Y':4}\n",
    "#         periods = { '2Y':2 }\n",
    "        \n",
    "#         num = list(periods.values())\n",
    "#         name = list(periods.keys())\n",
    "        \n",
    "#         dfy = pd.DataFrame()\n",
    "        \n",
    "#         for i in range(len(periods)):\n",
    "#             l = df5.rolling(window=int(52*num[i])).mean().iloc[[-1]].T\n",
    "#             dfy = pd.concat([dfy,l],axis=1)\n",
    "        \n",
    "#         dfy.columns = list(periods.keys())\n",
    "        \n",
    "#         dfz = pd.DataFrame()\n",
    "#         for i in range(len(tick)):\n",
    "#             dfz = pd.concat([dfz, globals()[f'df_{i}'].iloc[:,[-1]]],axis=1)\n",
    "#         dfz = dfz.sort_index()\n",
    "#         dfy1 = pd.DataFrame()\n",
    "        \n",
    "#         for i in range(len(periods)):\n",
    "#             l = dfz.rolling(window=int(52*num[i])).mean().iloc[[-1]].T\n",
    "#             l.columns = [name[i]]\n",
    "#             dfy1 = pd.concat([dfy1,l],axis=1)\n",
    "        \n",
    "#         dfA = dfy.copy()\n",
    "#         dfB = dfy1.copy()\n",
    "        \n",
    "#         dfy = dfy.astype(float).round(2)\n",
    "#         dfy1 = dfy1.astype(float).round(2)\n",
    "        \n",
    "#         dfy = dfy.astype(str)\n",
    "#         dfy1 = dfy1.astype(str)\n",
    "        \n",
    "#         rows = len(dfy1)\n",
    "#         columns = len(dfy1.columns)\n",
    "#         for i in range(rows):\n",
    "#             for j in range(columns):\n",
    "#                 st = '[' + str(dfy.iloc[i,j]) + ', '+ str(dfy1.iloc[i,j]) + ']'\n",
    "#                 dfy.iloc[i,j] = st\n",
    "                \n",
    "#         x21 = dfy[['2Y']].T.copy()\n",
    "#         x21.index.name = 'Date'\n",
    "#         x21.columns.name = ''\n",
    "#         x21.index = [backtest_last_date]\n",
    "\n",
    "#         backtest_last_date += timedelta(days=1)\n",
    "#     except Exception as e:\n",
    "#         x21 = None\n",
    "\n",
    "#     all_beta_df = pd.concat([all_beta_df,x21])\n",
    "\n",
    "# all_beta_df.index = pd.to_datetime(all_beta_df.index)\n",
    "# col_list = list(all_beta_df.columns)\n",
    "# col_list = [item.replace(\"ER \",\"\").replace(\" TR \",\"\").replace(\" vs.\",\"_\")\\\n",
    "#             .replace(\"LT03TRUU\",\"IEI\").replace(\"LT09TRUU\",\"IEF\")for item in col_list]\n",
    "# col_list1 = [item.split(\" - \")[0]+'_'+item.split(\" *\")[1] for item in col_list]\n",
    "# all_beta_df1 = all_beta_df.copy()\n",
    "\n",
    "# all_beta_df1.columns = col_list1\n",
    "# x = pd.concat([all_beta_df1,df_beta])\n",
    "# x = x.sort_index()\n",
    "# x = x[~x.index.duplicated(keep='last')]\n",
    "# x = x.dropna(axis=1)\n",
    "# last_beta = x.index[-1]\n",
    "# x.loc[last_beta+timedelta(days=1)] = x.iloc[-1,:].to_list()\n",
    "# x.loc[last_beta+timedelta(days=2)] = x.iloc[-1,:].to_list()\n",
    "# x.index.name = 'Date'\n",
    "# x.to_csv(\"All Basis Trade Betas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94fccbf4-98ba-424f-90ae-ca08b44f4161",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ER Code\n",
    "####################################################\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "        \"LQD Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDLIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"HYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"IEAC Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"IHYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"Fed Fund\": \"FF\",\n",
    "        \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "        \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "dq = df.copy()\n",
    "\n",
    "# dq['Fed Fund'] = dq['Fed Fund'].ffill()\n",
    "\n",
    "end_date = dq.index[-1]\n",
    "####################################### BBG Data Acquisition\n",
    "\n",
    "securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "              'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IBCN GR EQUITY',\n",
    "              'IEI US Equity','IEF US Equity']\n",
    "\n",
    "fields1 = ['YAS_MOD_DUR']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields1)\n",
    "df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "df1 = df.copy()\n",
    "\n",
    "#################################### Fixing Bad Data Point in YAS of IEI\n",
    "rolling_avg = df1['IEI DUR'].replace(0, np.nan).rolling(window=30, min_periods=1).mean()\n",
    "df1['IEI DUR'] = df1.apply(\n",
    "    lambda row: rolling_avg[row.name] if row['IEI DUR'] == 0.0 else row['IEI DUR'], axis=1\n",
    ")\n",
    "#################################### Fixing Bad Data Point in YAS of IEI\n",
    "\n",
    "securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "              'IEI US Equity','IEF US Equity', 'RSP US EQUITY', #'SPX INDEX',  'RTY INDEX',\n",
    "              'IBCN GR EQUITY',\n",
    "              'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY',\n",
    "              'GSCBHYEQ Index', 'GSCBIGEQ Index', 'SPY US EQUITY', 'EEM US EQUITY', 'IWM US EQUITY', 'IJH US EQUITY',\n",
    "             ]\n",
    "\n",
    "fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['QW3I INDEX']\n",
    "fields = ['MODIFIED_DURATION']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']  ############## I want to calculate funding rate for spx, rty and sx5e separately\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['EURR002W Index']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['ECB Rate']\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "bbg = df1.copy()\n",
    "dq.index = pd.to_datetime(dq.index)\n",
    "dq.index = dq.index.date\n",
    "bbg.index = pd.to_datetime(bbg.index)\n",
    "bbg.index = bbg.index.date\n",
    "\n",
    "data = pd.concat([dq,bbg],axis=1)\n",
    "data = data.sort_index()\n",
    "\n",
    "df_funding = data[[col for col in data.columns if ('Funding Sprd' in col)]+['Fed Fund']+['ECB Rate']]\n",
    "\n",
    "if np.isnan(df_funding.loc[df_funding.index[-1],'Fed Fund']):\n",
    "    df_funding.loc[df_funding.index[-1],'Fed Fund'] = df_funding.loc[df_funding.index[-2],'Fed Fund']\n",
    "\n",
    "# df_funding['Fed Fund'] = df_funding['Fed Fund'].ffill()\n",
    "\n",
    "for col in df_funding:\n",
    "    if col.endswith('Sprd'):\n",
    "        if col.split(' ')[0] in ['HYG','LQD']:\n",
    "            df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "            df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "        if col.split(' ')[0] in ['IHYG','IEAC']:\n",
    "            df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "            df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "\n",
    "df_funding['Net Long VCIT Funding'] = df_funding['Net Long LQD Funding']\n",
    "df_funding['Net Short VCIT Funding'] = df_funding['Net Short LQD Funding']\n",
    "\n",
    "for item in ['EMB','EEM']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.5\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.5\n",
    "\n",
    "for item in ['IEI', 'IEF', 'RSP', 'BKLN', 'GSCBHYEQ', 'GSCBIGEQ', 'SPX', 'RTY', 'SPY', 'IWM', 'IJH']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.15\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.15\n",
    "\n",
    "for item in ['IBCN','SX5E']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['ECB Rate'] + 0.15\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['ECB Rate'] - 0.15\n",
    "\n",
    "df_funding = df_funding[[col for col in df_funding.columns if col.startswith(\"Net\")]]\n",
    "df_funding.index = pd.to_datetime(df_funding.index)\n",
    "df_funding = df_funding.resample('D').last().ffill()\n",
    "\n",
    "original_er_data = data[[col for col in data.columns if col.startswith(\"ER \")]]\n",
    "tr_data = data[[col for col in data.columns if col.startswith(\"TR \")]]\n",
    "ust = tr_data[['TR LT09TRUU']] # for using corr later\n",
    "tr_data = tr_data.iloc[:,:-3] #dropping LT03/09 and QW3I\n",
    "\n",
    "tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "er_tr_data = er_tr_data.sort_index()\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "for item in etfs:\n",
    "    check = er_tr_data[item].dropna()\n",
    "    check = check.diff()/check.shift()\n",
    "    check = check.reindex(er_tr_data.index)\n",
    "    er_tr_data[item] = check\n",
    "    \n",
    "er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "############################################################### Funding Sprds\n",
    "funding = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "x = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "x = x.interpolate()\n",
    "x.to_excel(\"Funding Rates.xlsx\")\n",
    "\n",
    "y = x.copy()\n",
    "y = round(y,2)\n",
    "y.to_excel(\"Funding Rates 2.xlsx\")\n",
    "\n",
    "###############################################################\n",
    "for item in etfs:\n",
    "    name = item.split(' ')[1]\n",
    "    er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "                (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "\n",
    "er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "er_tr_data = er_tr_data.sort_index()\n",
    "\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "for item in etfs:\n",
    "    check = er_tr_data[item].dropna()\n",
    "    check = check.diff()/check.shift()\n",
    "    check = check.reindex(er_tr_data.index)\n",
    "    er_tr_data[item] = check\n",
    "\n",
    "er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "for item in etfs:\n",
    "    name = item.split(' ')[1]\n",
    "    er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "                (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "er_data = pd.concat([original_er_data,er_tr_data],axis=1)\n",
    "# er_data = er_data.dropna()\n",
    "# er_data.columns = er_data.columns.str.replace(\"ER SPX\",\"ER ESA\").str.replace(\"ER RTY\",\"ER RTYA\").str.replace(\"ER SX5E\",\"ER VGA\")\n",
    "er_data.columns = er_data.columns.str.replace(\"ER GSCBHYEQ\",\"ER HY Eqty\").str.replace(\"ER GSCBIGEQ\",\"ER IG Eqty\")\n",
    "er_data = er_data.sort_index()\n",
    "\n",
    "securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = er_data.index[0], flds = fields)\n",
    "df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "er_data = pd.concat([er_data,df], axis=1)\n",
    "er_data = er_data.sort_index()\n",
    "\n",
    "er_data.to_csv(\"All ER.csv\")\n",
    "\n",
    "##############################################################  Updating Durations\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "        \"CDX IG 5Y Dur\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "        \"CDX IG 10Y Dur\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "        \"ITRX XOVER 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX MAIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX MAIN 10Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "        \"ITRX SUBFIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX SNRFIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.loc[pd.to_datetime(datetime.now().date())] = [item - ((datetime.now() - df.index[-1]).days)/365 for item in list(df.iloc[-1])]\n",
    "df = df.interpolate()\n",
    "all_dq_dur = df.copy()\n",
    "all_dq_dur.to_excel(\"All DQ Duration.xlsx\")\n",
    "\n",
    "############################################################################## Updating Ref levels\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "    \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CLEAN_MID)\",\n",
    "    \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CLEAN_MID)\",\n",
    "\n",
    "    \"CDX IG 5Y DUR\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "    \"CDX IG 10Y DUR\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "    \"ITRX MAIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX MAIN 10Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "    \"ITRX SNRFIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX SUBFIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX XOVER 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.to_excel(\"Ref Levels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b0c5ad-9596-4892-8828-c7a78fc078e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_dq_dur = pd.read_excel(\"All DQ Duration.xlsx\")\n",
    "all_dq_dur['Date'] = pd.to_datetime(all_dq_dur['Date'])\n",
    "all_dq_dur = all_dq_dur.set_index('Date')\n",
    "\n",
    "dict_map = {\n",
    "# product type, start time, end time, carry (%), trades on sprd, slippage (bps or $),\n",
    "# fixed commission, notional (if selected as Y), BBG ticker, check live status using which ticker\n",
    "    'CDX IG 5Y': ['CDX', '07:30:00', '20:00:00', 1, 'Yes', 0.15, 500, 30*10**6, \"CDX IG CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX IG 10Y': ['CDX', '07:30:00', '20:00:00', 1, 'Yes', 0.3, 500, 30*10**6, \"CDX IG CDSI GEN 10Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX HY 5Y': ['CDX', '07:30:00', '20:00:00', 5, 'No', 0.02, 500, 6*10**6, \"CDX HY CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'SPX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"SPX INDEX\", \"ESA INDEX\"],\n",
    "    'SPY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"SPY US EQUITY\", \"ESA INDEX\"],\n",
    "    'IWM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IWM US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'RSP': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"RSP US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'RTY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"RTY INDEX\", \"RSP US EQUITY\"],\n",
    "    'IG Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"GSCBIGEQ Index\", \"RSP US EQUITY\"],\n",
    "    'HY Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"GSCBHYEQ Index\", \"RSP US EQUITY\"],\n",
    "    'ITRX MAIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"ITRX EUR CDSI GEN 5Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX MAIN 10Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.3, 500, 30*10**6, \"ITRX EUR CDSI GEN 10Y CORP\", \"ITRX EUR CDSI GEN 10Y CORP\"],\n",
    "    'ITRX SNRFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"SNRFIN CDSI GEN 5Y Corp\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX SUBFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"SUBFIN CDSI GEN 5Y Corp\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 5Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.15, 500, 6*10**6, \"ITRX XOVER CDSI GEN 5Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 10Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.3, 500, 6*10**6, \"ITRX XOVER CDSI GEN 10Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    # 'VIX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"VIX INDEX\", \"RSP US EQUITY\"],\n",
    "    # 'V2X': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"V2X INDEX\", \"SX5E INDEX\"],\n",
    "    'SX5E': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"SX5E INDEX\", \"SX5E INDEX\"],\n",
    "    'CDX EM 5Y': ['CDX', '07:30:00', '20:00:00', 1, 'No', 0.02, 500, 6*10**6, \"CDX EM CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'HYG': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"HYG US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'EMB': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"EMB US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'VCIT': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"VCIT US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'LQD': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"LQD US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IEI': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IEI US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IEF': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IEF US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'EEM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"EEM US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IJH': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IJH US EQUITY\", \"RSP US EQUITY\"],\n",
    "}\n",
    "\n",
    "l1 = list(dict_map.keys())\n",
    "l2 = [item[8] for item in list(dict_map.values())]\n",
    "reverse_dict = dict(zip(l2,l1))\n",
    "reverse_dict[\"ESA INDEX\"] = \"ESA\"\n",
    "\n",
    "live_dict = dict(zip([item[8] for item in dict_map.values()], [item[9] for item in dict_map.values()]))\n",
    "live_dict[\"ESA INDEX\"] = \"ESA INDEX\"\n",
    "\n",
    "last_checked = None\n",
    "current_date_esa_close = datetime.now()\n",
    "first_current_date_esa_close = True\n",
    "first_run = True\n",
    "\n",
    "df = pd.read_excel(\"Data for Credit-Eqty Dashboard v3.xlsx\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')\n",
    "df = df.sort_index()\n",
    "bbg_datafile = df.copy()\n",
    "last_update = df.dropna().index[-1]\n",
    "all_bbg_tickers = [dict_map[item][8] for item in df.columns]\n",
    "new_data = pd.DataFrame()\n",
    "\n",
    "for bbg_tickers in all_bbg_tickers:\n",
    "    bbg_date = pd.to_datetime(last_update.date())\n",
    "    data = None\n",
    "    while True:\n",
    "        try:\n",
    "            f = blp.bdib(ticker = bbg_tickers, dt = bbg_date, interval = 1, ref='IndexYieldCurve')      \n",
    "        except:\n",
    "            if bbg_date > pd.to_datetime(datetime.now().date()):\n",
    "                break\n",
    "        data = pd.concat([data,f])\n",
    "        bbg_date += timedelta(days=1)    \n",
    "    try:\n",
    "        data = data.iloc[:,[3]].copy()\n",
    "        data.columns = [bbg_tickers]\n",
    "    except:\n",
    "        continue\n",
    "    new_data = pd.concat([new_data, data],axis=1)\n",
    "\n",
    "new_data.index = new_data.index.tz_convert('America/New_York')\n",
    "new_data.index = new_data.index.tz_localize(None)\n",
    "\n",
    "live_list = ['CDX IG CDSI GEN 5Y CORP','ITRX EUR CDSI GEN 5Y CORP', 'RSP US EQUITY','SX5E INDEX','ESA INDEX']\n",
    "mask = blp.bdh(tickers = live_list, flds='PX_LAST', start_date = df.index[0]-timedelta(days=5))\n",
    "mask.columns = live_list\n",
    "mask.index = pd.to_datetime(mask.index).date\n",
    "\n",
    "new_data = new_data.ffill()\n",
    "new_data.index = pd.to_datetime(new_data.index)\n",
    "new_data.columns = [reverse_dict[item] for item in new_data.columns]\n",
    "\n",
    "bbg_datafile = bbg_datafile[bbg_datafile.index<new_data.index[0]].copy()\n",
    "bbg_datafile = pd.concat([bbg_datafile, new_data])\n",
    "bbg_datafile = bbg_datafile.sort_index()\n",
    "bbg_datafile.index.name = 'Date'\n",
    "bbg_datafile = bbg_datafile.ffill().copy()\n",
    "bbg_datafile = bbg_datafile[~bbg_datafile.index.duplicated(keep='last')]\n",
    "\n",
    "bbg_datafile['Date'] = pd.to_datetime(bbg_datafile.index.date)\n",
    "bbg_datafile['Time'] = bbg_datafile.index.time\n",
    "filtered_new_data = None\n",
    "\n",
    "for col in bbg_datafile.drop(['Date','Time'],axis=1).columns:\n",
    "    test = bbg_datafile[[col,'Date','Time']]\n",
    "    cond = (test['Date'].isin(mask[dict_map[col][9]].dropna().index)) & (test['Time']<=pd.\\\n",
    "            to_datetime(dict_map[col][2]).time()) & (test['Time']>=pd.to_datetime(dict_map[col][1]).time())\n",
    "    test = test[cond][[col]]\n",
    "    filtered_new_data = pd.concat([filtered_new_data, test],axis=1)\n",
    "\n",
    "bbg_datafile = filtered_new_data.copy()\n",
    "\n",
    "bbg_datafile = bbg_datafile.dropna(how='all')\n",
    "bbg_datafile.to_excel(\"Data for Credit-Eqty Dashboard v3.xlsx\")    \n",
    "master_bbg_datafile = bbg_datafile.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52886c5-8fc6-4571-be61-942a429d6a1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Ref Levels.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "ref = df[[col for col in df.columns if not col.endswith(\"DUR\")]]\n",
    "\n",
    "dur = df[[col for col in df.columns if col.endswith(\"DUR\")]]\n",
    "dur.loc[datetime.now()] = [np.nan] * len(dur.columns)\n",
    "dur = dur.resample(\"D\").last().ffill().copy()\n",
    "dur = dur.shift().resample(\"1min\").last().ffill().copy()  ############ yesterday's duration we take .. we have shifted it here\n",
    "dur.columns = [item.rsplit(\" \",1)[0] + '_dq_dur' for item in dur.columns]\n",
    "\n",
    "#########################################################################################\n",
    "bbg_tickers = [dict_map[item][8] for item in dict_map.keys()]\n",
    "# reverse_dict = dict(zip(bbg_tickers, list(dict_map.keys())))\n",
    "bbg_data = blp.bdh(tickers = bbg_tickers, flds='px_last', start_date='2017-01-01')\n",
    "bbg_data.columns = bbg_tickers\n",
    "bbg_data.index = pd.to_datetime(bbg_data.index)\n",
    "bbg_data.columns = [reverse_dict[item] for item in bbg_data.columns]\n",
    "\n",
    "for col in ref.columns:\n",
    "    bbg_data[col] = ref[col]\n",
    "\n",
    "bbg_data['ESA'] = bbg_data['SPY']\n",
    "\n",
    "bbg_data1 = bbg_data.resample(\"1min\").last().ffill().copy()\n",
    "bbg_data1.columns = [item +'_bbg_px' for item in bbg_data1.columns]\n",
    "\n",
    "bbg_data2 = bbg_data.shift().resample(\"1min\").last().ffill().copy()\n",
    "bbg_data2.columns = [item +'_bbg_px_2' for item in bbg_data2.columns]\n",
    "\n",
    "################################################## Creating SPY Imputed for recent day in BBG_DATFILE\n",
    "\n",
    "bbg_datafile[\"ESA\"] = bbg_datafile[\"SPY\"]\n",
    "\n",
    "last_close = None\n",
    "for tick in ['ESA INDEX','SPY US EQUITY']:\n",
    "    f = blp.bdib(ticker=tick, flds='PX_LAST', dt=str(list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-2]), ref='IndexUS')\n",
    "    f.index = f.index.tz_convert('America/New_York').tz_localize(None)\n",
    "    f = f.iloc[:,[3]]\n",
    "    f.columns = [tick]\n",
    "    last_close = pd.concat([last_close,f],axis=1)\n",
    "\n",
    "esa_val = last_close['ESA INDEX'].ffill().iloc[-1]\n",
    "spx_val = last_close['SPY US EQUITY'].iloc[-1]\n",
    "\n",
    "tick = \"ESA INDEX\"\n",
    "start_date = list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1]\n",
    "all_f = None\n",
    "for i in range(4):\n",
    "    try:\n",
    "        f = blp.bdib(ticker=tick, flds='PX_LAST', dt=str(start_date+timedelta(days=i)), ref='IndexYieldCurve')\n",
    "        f.index = f.index.tz_convert('America/New_York').tz_localize(None)\n",
    "        f = f.iloc[:,[3]]\n",
    "        f.columns = [tick]\n",
    "        all_f = pd.concat([f, all_f]).sort_index().copy()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "f = all_f.copy()\n",
    "f = f[f.index.date == pd.to_datetime(str(list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1])).date()]\n",
    "f['SPY Imputed'] = round((f/esa_val) * spx_val,2)\n",
    "f = f[['SPY Imputed']].copy()\n",
    "f.columns = ['ESA']\n",
    "\n",
    "recent_spy = bbg_datafile[bbg_datafile.index.date<list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1]][['ESA']]\n",
    "new_spy = pd.concat([recent_spy, f]).sort_index().copy()\n",
    "bbg_datafile = pd.concat([bbg_datafile.drop(\"ESA\",axis=1), new_spy], axis=1).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691bfc3a-471a-40f5-ae22-e9c4210f28d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = bbg_datafile.copy()\n",
    "er_data = pd.read_csv(\"All ER.csv\",index_col=0, parse_dates=True)\n",
    "\n",
    "er_data.loc[datetime.now().date()] = [np.nan] * len(er_data.columns)\n",
    "er_data.index = pd.to_datetime(er_data.index)\n",
    "er_data = er_data.resample(\"D\").last().ffill().copy()\n",
    "er_data.columns = [item.split(\"ER \",1)[1] for item in er_data.columns]\n",
    "er_data[['ESA']] = er_data[['SPY']]\n",
    "\n",
    "er = er_data.copy()\n",
    "er.columns = [item + '_dq_ER' for item in er.columns]\n",
    "er = er.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "er2 = er_data.shift().copy()\n",
    "er2.columns = [item + '_dq_ER_2' for item in er2.columns]\n",
    "er2 = er2.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "############################################################ Converting historical price series into historical er series\n",
    "\n",
    "intraday_tr_data = None\n",
    "\n",
    "for col in df.columns:    \n",
    "    first_run = False\n",
    "    if col == \"ESA\":\n",
    "        first_run = True\n",
    "    elif dict_map[col][0] == \"Eq\" or (dict_map[col][0] == \"CDX\" and dict_map[col][4] == 'No'):\n",
    "        first_run = True\n",
    "    \n",
    "    if first_run:\n",
    "        x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']],\\\n",
    "           bbg_data2[[f'{col}_bbg_px_2']],], axis=1).sort_index().dropna().copy()\n",
    "        \n",
    "        x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "        \n",
    "        if col in [\"CDX HY 5Y\", \"CDX HY 10Y\", \"CDX EM 5Y\"]:\n",
    "            x['d-o-d px pnl'] = (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "            x['intraday px pnl'] = (x[col] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "        else:\n",
    "            x['d-o-d px pnl'] = (x[f'{col}_bbg_px']/ x[f'{col}_bbg_px_2'] - 1)\n",
    "            x['intraday px pnl'] = (x[col] / x[f'{col}_bbg_px_2'] - 1)\n",
    "            \n",
    "        x['Calculated TR Change'] = x['TR Change'] - x['d-o-d px pnl'] + x['intraday px pnl']\n",
    "        # x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "        # x = x[['Actual TR Series']].copy()\n",
    "        # x.columns = [col]\n",
    "    \n",
    "    \n",
    "    elif dict_map[col][4] == 'Yes':\n",
    "        x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "            dur[[f'{col}_dq_dur']]], axis=1).sort_index()#.dropna().copy()\n",
    "        x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "        x['d-o-d sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "        x['intraday sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[col] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "        \n",
    "        x['Calculated TR Change'] = x['TR Change'] - x['d-o-d sprd pnl'] + x['intraday sprd pnl']\n",
    "    x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "    x = x[['Actual TR Series']].copy()\n",
    "    x.columns = [col]   \n",
    "    intraday_tr_data = pd.concat([intraday_tr_data, x], axis=1)\n",
    "\n",
    "######################################################################################### Getting ref levels from last close\n",
    "\n",
    "intraday_tr_data1 = intraday_tr_data[intraday_tr_data.index.date<datetime.now().date()].dropna(how='all').copy()\n",
    "last_dict = {}\n",
    "for col in intraday_tr_data1.columns:\n",
    "    last_ref = bbg_datafile.loc[intraday_tr_data1[[col]].dropna().index[-1]][col]\n",
    "    last_date = str(intraday_tr_data1[[col]].dropna().index[-1].date())\n",
    "    last_dict[col] = [intraday_tr_data1[col].dropna().iloc[-1], last_ref, last_date]\n",
    "\n",
    "\n",
    "######################################################################################### calculating today's er series\n",
    "funding = pd.read_excel(\"Funding Rates.xlsx\",index_col=0, parse_dates=True)\n",
    "funding.columns = funding.columns.str.replace(\"GSCBHYEQ\",\"HY Eqty\").str.replace(\"GSCBIGEQ\",\"IG Eqty\")\n",
    "funding['Net Long ESA Funding'] = funding['Net Long SPY Funding']\n",
    "funding['Net Short ESA Funding'] = funding['Net Short SPY Funding']\n",
    "\n",
    "recent_bbg_datafile = bbg_datafile[bbg_datafile.index.date == datetime.now().date()].copy()\n",
    "all_today_er_series = None\n",
    "\n",
    "for col in recent_bbg_datafile.columns:\n",
    "    carry_days = (datetime.now() - pd.to_datetime(last_dict[col][2])).days\n",
    "    x = recent_bbg_datafile[[col]].dropna().copy()\n",
    "    \n",
    "    first_run = False\n",
    "    if col==\"ESA\":\n",
    "        first_run = True\n",
    "    elif dict_map[col][0] == \"Eq\":\n",
    "        first_run = True\n",
    "        \n",
    "    if first_run:\n",
    "        rate = funding[[item for item in funding.columns if col == item.split(\" \",2)[2].rsplit(\" \",1)[0]]].dropna().iloc[-1]\n",
    "        rate = rate.mean()   ############ etf funding rate\n",
    "        x = (last_dict[col][0])*((x/last_dict[col][1])-(rate/100)*(carry_days/360))\n",
    "    elif dict_map[col][4] == \"Yes\":\n",
    "        x = (last_dict[col][0])*(1 - dur[f'{col}_dq_dur'].iloc[-1]*(x-last_dict[col][1])*(10**(-4))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "    elif dict_map[col][0] == \"CDX\":\n",
    "        x = (last_dict[col][0])*(1 + (x-last_dict[col][1])*(10**(-2))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "\n",
    "    all_today_er_series = pd.concat([all_today_er_series, x], axis=1)\n",
    "all_intraday_er_series = pd.concat([intraday_tr_data1, all_today_er_series]).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d53d2a3-7e40-4dd2-832b-1cbab3ada7af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    # 1 : [\"Intraday\",252,252,'A (Intraday; 12M)'],\n",
    "    # 2 : [\"Intraday\",315,315,'B (Intraday; 15M)'],\n",
    "    3 : [\"Intraday\",378,378,'C (Intraday; 18M)'],\n",
    "}\n",
    "\n",
    "def x_model(model_num, models_list, diff_var, calc_method):\n",
    "    all_zplots = None\n",
    "    val = []\n",
    "    \n",
    "    for global_model in models_list:    \n",
    "        model_Y = global_model[0]\n",
    "        model_X = global_model[1]\n",
    "        zscore_Y = global_model[2]\n",
    "        zscore_X = global_model[3]\n",
    "    \n",
    "        backtest_start_date = pd.to_datetime('2021-12-01')\n",
    "        \n",
    "        zscore_vars = [model_Y, zscore_Y] + model_X + zscore_X\n",
    "        zscore_vars = list(set(zscore_vars))\n",
    "        \n",
    "        if 'ESA' in zscore_vars:\n",
    "            zscore_vars_start_time = '07:30:00'\n",
    "            zscore_vars_end_time = '20:00:00'\n",
    "    \n",
    "        # elif ('ITRX XOVER 5Y' in zscore_vars) and ('CDX HY 5Y' in zscore_vars):\n",
    "        #     zscore_vars_start_time = '07:15:00'\n",
    "        #     zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "    \n",
    "        else:\n",
    "            zscore_vars_start_time = max([dict_map[item][1] for item in zscore_vars])\n",
    "            zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "        \n",
    "        ################################## Beta Calculation\n",
    "\n",
    "        # if len(model_X) == 1:\n",
    "        #     er_Y = f'ER {model_Y}'\n",
    "        #     er_X = f'ER {model_X[0]}'\n",
    "        #     er_data = pd.read_csv(\"All ER.csv\")\n",
    "        #     er_data.columns = ['Date'] + list(er_data.columns)[1:]\n",
    "        #     er_data['Date'] = pd.to_datetime(er_data['Date'])\n",
    "        #     er_data = er_data.set_index('Date')\n",
    "        #     er_data = er_data.sort_index()\n",
    "        #     beta = er_data[[er_Y, er_X]].dropna()\n",
    "        #     beta = beta.resample('W').last()\n",
    "        #     beta = np.log(beta)\n",
    "        #     beta = beta.diff().dropna()\n",
    "        #     beta['Beta1'] = [np.nan] * len(beta)\n",
    "        #     beta['Beta2'] = [np.nan] * len(beta)\n",
    "            \n",
    "        #     for i in range(len(beta)-24+1):\n",
    "        #         reg_X = beta[er_X].iloc[i:i+24]\n",
    "        #         reg_Y = beta[er_Y].iloc[i:i+24]\n",
    "        #         model = sm.OLS(reg_Y, sm.add_constant(reg_X)).fit() \n",
    "        #         beta.iloc[i+23,2] = model.params.iloc[1]\n",
    "            \n",
    "        #         model = sm.OLS(reg_X, sm.add_constant(reg_Y)).fit() \n",
    "        #         beta.iloc[i+23,3] = model.params.iloc[1]\n",
    "            \n",
    "        #     beta['Beta1'] = beta['Beta1'].rolling(104).mean()\n",
    "        #     beta['Beta2'] = beta['Beta2'].rolling(104).mean()\n",
    "        #     beta['Beta'] = 0.5*(beta['Beta1'] + 1/ beta['Beta2'])\n",
    "        #     beta = beta[['Beta']].dropna()\n",
    "        # else:\n",
    "        #     ############################################################# Update this beta calculation\n",
    "    \n",
    "        #     b1 = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "        #     b1.columns = ['Date'] + list(b1.columns)[1:]\n",
    "        #     b1 = b1.set_index('Date')\n",
    "        #     beta = b1[[f'{model_Y}_{model_X[0]}_{model_X[1]}']]\n",
    "        #     beta.columns = ['Beta']\n",
    "        #     beta['Coef1'] = beta['Beta'].apply(lambda x: eval(x)[0])\n",
    "        #     beta['Coef2'] = beta['Beta'].apply(lambda x: eval(x)[1])\n",
    "        #     beta.index = pd.to_datetime(beta.index)\n",
    "            \n",
    "        # # beta = beta.resample(sampling_freq).first().ffill()\n",
    "        # beta = beta.resample(\"1min\").first().ffill()\n",
    "        \n",
    "        beta = None\n",
    "        \n",
    "        ######################################## Getting data from the master file......\n",
    "        if calc_method == \"Price\":\n",
    "            df = bbg_datafile.copy()    \n",
    "        elif calc_method == \"Return\":\n",
    "            df = all_intraday_er_series.copy()\n",
    "        \n",
    "        zscore_df = df[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).copy()\n",
    "        zscore_df3 = zscore_df.copy()\n",
    "        valid_dates = zscore_df3.index.date\n",
    "        zscore_df3 = zscore_df3.iloc[-6000:].resample(\"1min\").last().ffill().copy()\n",
    "        zscore_df3 = zscore_df3[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df3 = zscore_df3[pd.Series(zscore_df3.index.date, index=zscore_df3.index).isin(valid_dates)]\n",
    "        zscore_df3 = zscore_df3[zscore_df3.index <= pd.to_datetime(bbg_time)]\n",
    "\n",
    "        start_check = pd.to_datetime(zscore_vars_start_time) - timedelta(minutes=7)\n",
    "        start_check = str(start_check.time())\n",
    "        zscore_df = zscore_df.resample(\"10min\", offset=\"5min\").last().ffill().copy()\n",
    "        zscore_df = zscore_df[zscore_vars].between_time(start_check, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df = zscore_df[pd.Series(zscore_df.index.date, index=zscore_df.index).isin(valid_dates)]\n",
    "        zscore_df = zscore_df[zscore_df.index >= backtest_start_date]\n",
    "        zscore_df = zscore_df[zscore_df.index <= pd.to_datetime(bbg_time)]\n",
    "        \n",
    "        check_later = zscore_df.copy()\n",
    "        sampling_multiplier = len(set(list(check_later.index.time)))\n",
    "        \n",
    "        \n",
    "        ################################## ZScore Calculation Start : Convert Sprd to PX series\n",
    "        \n",
    "        if calc_method == \"Price\":\n",
    "            zscore_df1 = zscore_df.copy()\n",
    "            df = all_dq_dur.copy()\n",
    "            df.columns = df.columns.str.replace(\" Dur\",\"\")\n",
    "            \n",
    "            last_dq_date = df.index[-1]\n",
    "            last_value = df.iloc[-1,0]\n",
    "            \n",
    "            df.loc[ last_dq_date + timedelta(days=1) ] = last_value\n",
    "            df.loc[ last_dq_date + timedelta(days=2) ] = last_value\n",
    "            \n",
    "            df = df.resample(\"1min\").first().ffill().dropna()\n",
    "            dq_dur = df.copy()\n",
    "            \n",
    "            for col in zscore_df1.columns:\n",
    "                if col in dq_dur.columns:\n",
    "                    zscore_df1[f'{col} Dur'] = dq_dur[col]\n",
    "                    zscore_df1[f'{col} Dur'] = zscore_df1[f'{col} Dur'].shift(1)\n",
    "                    zscore_df1[f'Diff {col}'] = zscore_df1[col].diff()\n",
    "                    zscore_df1 = zscore_df1.dropna()\n",
    "                    zscore_df1[f'{col} Daily PX Change'] = -1 * zscore_df1[f'Diff {col}'] * zscore_df1[f'{col} Dur'] *10**(-4)\n",
    "                    zscore_df1[f'{col} Sum PX'] = zscore_df1[f'{col} Daily PX Change'].cumsum()\n",
    "                    zscore_df1[col] = zscore_df1[f'{col} Sum PX']\n",
    "                    zscore_df1 = zscore_df1[zscore_df.columns].copy()\n",
    "        elif calc_method == \"Return\":\n",
    "            zscore_df1 = zscore_df.copy()\n",
    "        \n",
    "        ################################## ZScore Calculation: Differencing and converting to ZScores\n",
    "        \n",
    "        zscore_df = zscore_df1[zscore_df1.index >= backtest_start_date].copy()\n",
    "        \n",
    "        col_list = zscore_df.columns\n",
    "        for period in diff_var:\n",
    "            for col in col_list:\n",
    "                zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*5*period)\n",
    "        \n",
    "        model_lookback = sampling_multiplier*dict_models[model_num][1]\n",
    "        model_lookback_res = sampling_multiplier*dict_models[model_num][2]\n",
    "        \n",
    "        zscore_df = zscore_df.dropna().copy()\n",
    "        zscore_df2 = zscore_df.copy()\n",
    "    \n",
    "        for period in diff_var:\n",
    "            i = len(zscore_df) - model_lookback\n",
    "            reg_Y = zscore_df[[f'{zscore_Y}_{period}W']].iloc[i:i+model_lookback]\n",
    "            reg_X = zscore_df[[item + f\"_{period}W\" for item in zscore_X]].iloc[i:i+model_lookback]                        \n",
    "            model = sm.OLS(reg_Y,sm.add_constant(reg_X)).fit()\n",
    "            # x = (model.resid - model.resid.rolling(model_lookback_res).mean())/model.resid.rolling(model_lookback_res).std()\n",
    "            x = (model.resid - model.resid.mean())/model.resid.std()\n",
    "            zscore_df.loc[zscore_df.index[i+model_lookback-1],f'{period}W_ZScore'] = x.iloc[-1]\n",
    "        \n",
    "        zscore_df['Avg. ZScore'] = zscore_df[[col for col in zscore_df.columns if col.endswith(\"_ZScore\")]].mean(axis=1)\n",
    "        zscore_df = zscore_df[['Avg. ZScore']]\n",
    "    \n",
    "        bt_df = pd.concat([check_later[[model_Y] + model_X],zscore_df],axis=1).dropna()\n",
    "        bt_df = pd.concat([bt_df,beta],axis=1).dropna()\n",
    "        \n",
    "        zplot = bt_df[['Avg. ZScore']].copy()  \n",
    "        zscore_df3 = zscore_df3.dropna()\n",
    "        val += [zscore_df3.index[-1]]\n",
    "        \n",
    "        if calc_method == \"Price\":\n",
    "            if len(model_X)==1:\n",
    "                zplot.columns = [f'{model_Y}({zscore_df3.iloc[-1][model_Y]}) vs. {model_X[0]}({zscore_df3.\\\n",
    "                iloc[-1][model_X[0]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "            else:\n",
    "                zplot.columns = [f'{model_Y}({zscore_df3.\\\n",
    "                iloc[-1][model_Y]}) vs. {model_X}({zscore_df3.iloc[-1][model_X[0]]} & {zscore_df3.\\\n",
    "                iloc[-1][model_X[1]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "        \n",
    "        elif calc_method == \"Return\":\n",
    "            \n",
    "            px_sprd_ref = bbg_datafile.iloc[-9000:].copy()\n",
    "            px_sprd_ref = px_sprd_ref.resample(\"1min\").last().ffill().sort_index().copy()\n",
    "            \n",
    "            if len(model_X)==1:\n",
    "                zplot.columns = [f'{model_Y}({px_sprd_ref.loc[zscore_df3.index[-1], model_Y]}) vs. {model_X[0]}({px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_X[0]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "            else:\n",
    "                zplot.columns = [f'{model_Y}({px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_Y]}) vs. {model_X}({px_sprd_ref.loc[zscore_df3.index[-1], model_X[0]]} & {px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_X[1]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "\n",
    "        all_zplots = pd.concat([all_zplots, zplot],axis=1)\n",
    "        zplot = all_zplots.copy()\n",
    "    return zplot, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548e4771-c543-40df-a1f6-3432d068c63a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_abbe0_row0_col4 {\n",
       "  background-color: rgba(243,200,190,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row0_col5 {\n",
       "  background-color: rgba(248,222,217,0.75);\n",
       "}\n",
       "#T_abbe0_row1_col4 {\n",
       "  background-color: rgba(249,228,224,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row1_col5 {\n",
       "  background-color: rgba(230,135,115,0.75);\n",
       "}\n",
       "#T_abbe0_row2_col0, #T_abbe0_row2_col1, #T_abbe0_row2_col2, #T_abbe0_row2_col3, #T_abbe0_row2_col6, #T_abbe0_row2_col7, #T_abbe0_row7_col0, #T_abbe0_row7_col1, #T_abbe0_row7_col2, #T_abbe0_row7_col3, #T_abbe0_row7_col6, #T_abbe0_row7_col7, #T_abbe0_row13_col0, #T_abbe0_row13_col1, #T_abbe0_row13_col2, #T_abbe0_row13_col3, #T_abbe0_row13_col6, #T_abbe0_row13_col7, #T_abbe0_row15_col0, #T_abbe0_row15_col1, #T_abbe0_row15_col2, #T_abbe0_row15_col3, #T_abbe0_row15_col6, #T_abbe0_row15_col7, #T_abbe0_row17_col0, #T_abbe0_row17_col1, #T_abbe0_row17_col2, #T_abbe0_row17_col3, #T_abbe0_row17_col6, #T_abbe0_row17_col7, #T_abbe0_row21_col0, #T_abbe0_row21_col1, #T_abbe0_row21_col2, #T_abbe0_row21_col3, #T_abbe0_row21_col6, #T_abbe0_row21_col7 {\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row2_col4 {\n",
       "  background-color: rgba(95,190,143,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row2_col5 {\n",
       "  background-color: rgba(165,218,192,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row3_col4, #T_abbe0_row4_col4, #T_abbe0_row16_col4 {\n",
       "  background-color: rgba(240,183,171,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row3_col5 {\n",
       "  background-color: rgba(177,223,201,0.75);\n",
       "}\n",
       "#T_abbe0_row4_col5 {\n",
       "  background-color: rgba(175,222,199,0.75);\n",
       "}\n",
       "#T_abbe0_row5_col4 {\n",
       "  background-color: rgba(248,224,219,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row5_col5, #T_abbe0_row20_col5 {\n",
       "  background-color: rgba(237,248,243,0.75);\n",
       "}\n",
       "#T_abbe0_row6_col4 {\n",
       "  background-color: rgba(246,216,209,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row6_col5 {\n",
       "  background-color: rgba(246,251,249,0.75);\n",
       "}\n",
       "#T_abbe0_row7_col4 {\n",
       "  background-color: rgba(249,230,226,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row7_col5 {\n",
       "  background-color: rgba(213,238,225,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row8_col4 {\n",
       "  background-color: rgba(189,228,209,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row8_col5 {\n",
       "  background-color: rgba(170,220,196,0.75);\n",
       "}\n",
       "#T_abbe0_row9_col4 {\n",
       "  background-color: rgba(181,225,203,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row9_col5 {\n",
       "  background-color: rgba(167,219,194,0.75);\n",
       "}\n",
       "#T_abbe0_row10_col4 {\n",
       "  background-color: rgba(99,191,146,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row10_col5 {\n",
       "  background-color: rgba(242,250,246,0.75);\n",
       "}\n",
       "#T_abbe0_row11_col4 {\n",
       "  background-color: rgba(140,208,175,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row11_col5 {\n",
       "  background-color: rgba(250,253,251,0.75);\n",
       "}\n",
       "#T_abbe0_row12_col4 {\n",
       "  background-color: rgba(253,246,245,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row12_col5 {\n",
       "  background-color: rgba(118,199,160,0.75);\n",
       "}\n",
       "#T_abbe0_row13_col4 {\n",
       "  background-color: rgba(87,187,138,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row13_col5 {\n",
       "  background-color: rgba(183,226,205,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row14_col4 {\n",
       "  background-color: rgba(246,212,205,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row14_col5 {\n",
       "  background-color: rgba(180,224,202,0.75);\n",
       "}\n",
       "#T_abbe0_row15_col4 {\n",
       "  background-color: rgba(193,230,212,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row15_col5 {\n",
       "  background-color: rgba(176,223,200,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row16_col5 {\n",
       "  background-color: rgba(248,225,220,0.75);\n",
       "}\n",
       "#T_abbe0_row17_col4 {\n",
       "  background-color: rgba(245,208,200,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row17_col5 {\n",
       "  background-color: rgba(254,254,254,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row18_col4 {\n",
       "  background-color: rgba(230,135,115,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row18_col5 {\n",
       "  background-color: rgba(221,241,231,0.75);\n",
       "}\n",
       "#T_abbe0_row19_col4 {\n",
       "  background-color: rgba(152,213,183,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row19_col5 {\n",
       "  background-color: rgba(245,251,248,0.75);\n",
       "}\n",
       "#T_abbe0_row20_col4 {\n",
       "  background-color: rgba(252,244,243,0.75);\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_abbe0_row21_col4 {\n",
       "  background-color: rgba(197,231,215,0.75);\n",
       "  font-weight: bold;\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "#T_abbe0_row21_col5 {\n",
       "  background-color: rgba(87,187,138,0.75);\n",
       "  border-bottom: 3px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_abbe0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_abbe0_level0_col0\" class=\"col_heading level0 col0\" colspan=\"8\">11:26 AM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_abbe0_level1_col0\" class=\"col_heading level1 col0\" >Long Risk</th>\n",
       "      <th id=\"T_abbe0_level1_col1\" class=\"col_heading level1 col1\" >Ref</th>\n",
       "      <th id=\"T_abbe0_level1_col2\" class=\"col_heading level1 col2\" >Short Risk</th>\n",
       "      <th id=\"T_abbe0_level1_col3\" class=\"col_heading level1 col3\" >Ref.</th>\n",
       "      <th id=\"T_abbe0_level1_col4\" class=\"col_heading level1 col4\" >PX_Zscore 1/2/3w</th>\n",
       "      <th id=\"T_abbe0_level1_col5\" class=\"col_heading level1 col5\" >Rtn_Zscore 6/9/12w</th>\n",
       "      <th id=\"T_abbe0_level1_col6\" class=\"col_heading level1 col6\" >Valuation</th>\n",
       "      <th id=\"T_abbe0_level1_col7\" class=\"col_heading level1 col7\" >Last Update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row0_col0\" class=\"data row0 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row0_col1\" class=\"data row0 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row0_col2\" class=\"data row0 col2\" >['VCIT', 'IEF']</td>\n",
       "      <td id=\"T_abbe0_row0_col3\" class=\"data row0 col3\" >['84.6125', '97.4545']</td>\n",
       "      <td id=\"T_abbe0_row0_col4\" class=\"data row0 col4\" >0.27</td>\n",
       "      <td id=\"T_abbe0_row0_col5\" class=\"data row0 col5\" >0.54</td>\n",
       "      <td id=\"T_abbe0_row0_col6\" class=\"data row0 col6\" >IG rich to ['VCIT', 'IEF']</td>\n",
       "      <td id=\"T_abbe0_row0_col7\" class=\"data row0 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row1_col0\" class=\"data row1 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row1_col1\" class=\"data row1 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row1_col2\" class=\"data row1 col2\" >['HYG', 'IEI']</td>\n",
       "      <td id=\"T_abbe0_row1_col3\" class=\"data row1 col3\" >['80.735', '120.185']</td>\n",
       "      <td id=\"T_abbe0_row1_col4\" class=\"data row1 col4\" >0.13</td>\n",
       "      <td id=\"T_abbe0_row1_col5\" class=\"data row1 col5\" >1.99</td>\n",
       "      <td id=\"T_abbe0_row1_col6\" class=\"data row1 col6\" >HY rich to ['HYG', 'IEI']</td>\n",
       "      <td id=\"T_abbe0_row1_col7\" class=\"data row1 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row2_col0\" class=\"data row2 col0\" >EM</td>\n",
       "      <td id=\"T_abbe0_row2_col1\" class=\"data row2 col1\" >97.74338</td>\n",
       "      <td id=\"T_abbe0_row2_col2\" class=\"data row2 col2\" >['EMB', 'IEF']</td>\n",
       "      <td id=\"T_abbe0_row2_col3\" class=\"data row2 col3\" >['96.03', '97.4545']</td>\n",
       "      <td id=\"T_abbe0_row2_col4\" class=\"data row2 col4\" >-0.39</td>\n",
       "      <td id=\"T_abbe0_row2_col5\" class=\"data row2 col5\" >-0.73</td>\n",
       "      <td id=\"T_abbe0_row2_col6\" class=\"data row2 col6\" >EM cheap to ['EMB', 'IEF']</td>\n",
       "      <td id=\"T_abbe0_row2_col7\" class=\"data row2 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row3_col0\" class=\"data row3 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row3_col1\" class=\"data row3 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row3_col2\" class=\"data row3 col2\" >ESA</td>\n",
       "      <td id=\"T_abbe0_row3_col3\" class=\"data row3 col3\" >669.42</td>\n",
       "      <td id=\"T_abbe0_row3_col4\" class=\"data row3 col4\" >0.35</td>\n",
       "      <td id=\"T_abbe0_row3_col5\" class=\"data row3 col5\" >-0.63</td>\n",
       "      <td id=\"T_abbe0_row3_col6\" class=\"data row3 col6\" >IG rich to ESA</td>\n",
       "      <td id=\"T_abbe0_row3_col7\" class=\"data row3 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row4_col0\" class=\"data row4 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row4_col1\" class=\"data row4 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row4_col2\" class=\"data row4 col2\" >SPY</td>\n",
       "      <td id=\"T_abbe0_row4_col3\" class=\"data row4 col3\" >669.56</td>\n",
       "      <td id=\"T_abbe0_row4_col4\" class=\"data row4 col4\" >0.35</td>\n",
       "      <td id=\"T_abbe0_row4_col5\" class=\"data row4 col5\" >-0.65</td>\n",
       "      <td id=\"T_abbe0_row4_col6\" class=\"data row4 col6\" >IG rich to SPY</td>\n",
       "      <td id=\"T_abbe0_row4_col7\" class=\"data row4 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row5_col0\" class=\"data row5 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row5_col1\" class=\"data row5 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row5_col2\" class=\"data row5 col2\" >RSP</td>\n",
       "      <td id=\"T_abbe0_row5_col3\" class=\"data row5 col3\" >189.91</td>\n",
       "      <td id=\"T_abbe0_row5_col4\" class=\"data row5 col4\" >0.15</td>\n",
       "      <td id=\"T_abbe0_row5_col5\" class=\"data row5 col5\" >-0.14</td>\n",
       "      <td id=\"T_abbe0_row5_col6\" class=\"data row5 col6\" >IG rich to RSP</td>\n",
       "      <td id=\"T_abbe0_row5_col7\" class=\"data row5 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row6_col0\" class=\"data row6 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row6_col1\" class=\"data row6 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row6_col2\" class=\"data row6 col2\" >IJH</td>\n",
       "      <td id=\"T_abbe0_row6_col3\" class=\"data row6 col3\" >65.16</td>\n",
       "      <td id=\"T_abbe0_row6_col4\" class=\"data row6 col4\" >0.19</td>\n",
       "      <td id=\"T_abbe0_row6_col5\" class=\"data row6 col5\" >-0.07</td>\n",
       "      <td id=\"T_abbe0_row6_col6\" class=\"data row6 col6\" >IG rich to IJH</td>\n",
       "      <td id=\"T_abbe0_row6_col7\" class=\"data row6 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row7_col0\" class=\"data row7 col0\" >IG</td>\n",
       "      <td id=\"T_abbe0_row7_col1\" class=\"data row7 col1\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row7_col2\" class=\"data row7 col2\" >IG Eqty</td>\n",
       "      <td id=\"T_abbe0_row7_col3\" class=\"data row7 col3\" >206.68</td>\n",
       "      <td id=\"T_abbe0_row7_col4\" class=\"data row7 col4\" >0.12</td>\n",
       "      <td id=\"T_abbe0_row7_col5\" class=\"data row7 col5\" >-0.34</td>\n",
       "      <td id=\"T_abbe0_row7_col6\" class=\"data row7 col6\" >IG rich to IG Eqty</td>\n",
       "      <td id=\"T_abbe0_row7_col7\" class=\"data row7 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row8_col0\" class=\"data row8 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row8_col1\" class=\"data row8 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row8_col2\" class=\"data row8 col2\" >ESA</td>\n",
       "      <td id=\"T_abbe0_row8_col3\" class=\"data row8 col3\" >669.42</td>\n",
       "      <td id=\"T_abbe0_row8_col4\" class=\"data row8 col4\" >-0.16</td>\n",
       "      <td id=\"T_abbe0_row8_col5\" class=\"data row8 col5\" >-0.69</td>\n",
       "      <td id=\"T_abbe0_row8_col6\" class=\"data row8 col6\" >HY cheap to ESA</td>\n",
       "      <td id=\"T_abbe0_row8_col7\" class=\"data row8 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row9_col0\" class=\"data row9 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row9_col1\" class=\"data row9 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row9_col2\" class=\"data row9 col2\" >SPY</td>\n",
       "      <td id=\"T_abbe0_row9_col3\" class=\"data row9 col3\" >669.56</td>\n",
       "      <td id=\"T_abbe0_row9_col4\" class=\"data row9 col4\" >-0.18</td>\n",
       "      <td id=\"T_abbe0_row9_col5\" class=\"data row9 col5\" >-0.71</td>\n",
       "      <td id=\"T_abbe0_row9_col6\" class=\"data row9 col6\" >HY cheap to SPY</td>\n",
       "      <td id=\"T_abbe0_row9_col7\" class=\"data row9 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row10_col0\" class=\"data row10 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row10_col1\" class=\"data row10 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row10_col2\" class=\"data row10 col2\" >RSP</td>\n",
       "      <td id=\"T_abbe0_row10_col3\" class=\"data row10 col3\" >189.91</td>\n",
       "      <td id=\"T_abbe0_row10_col4\" class=\"data row10 col4\" >-0.38</td>\n",
       "      <td id=\"T_abbe0_row10_col5\" class=\"data row10 col5\" >-0.10</td>\n",
       "      <td id=\"T_abbe0_row10_col6\" class=\"data row10 col6\" >HY cheap to RSP</td>\n",
       "      <td id=\"T_abbe0_row10_col7\" class=\"data row10 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row11_col0\" class=\"data row11 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row11_col1\" class=\"data row11 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row11_col2\" class=\"data row11 col2\" >IJH</td>\n",
       "      <td id=\"T_abbe0_row11_col3\" class=\"data row11 col3\" >65.16</td>\n",
       "      <td id=\"T_abbe0_row11_col4\" class=\"data row11 col4\" >-0.28</td>\n",
       "      <td id=\"T_abbe0_row11_col5\" class=\"data row11 col5\" >-0.04</td>\n",
       "      <td id=\"T_abbe0_row11_col6\" class=\"data row11 col6\" >HY cheap to IJH</td>\n",
       "      <td id=\"T_abbe0_row11_col7\" class=\"data row11 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row12_col0\" class=\"data row12 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row12_col1\" class=\"data row12 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row12_col2\" class=\"data row12 col2\" >IWM</td>\n",
       "      <td id=\"T_abbe0_row12_col3\" class=\"data row12 col3\" >244.83</td>\n",
       "      <td id=\"T_abbe0_row12_col4\" class=\"data row12 col4\" >0.04</td>\n",
       "      <td id=\"T_abbe0_row12_col5\" class=\"data row12 col5\" >-1.11</td>\n",
       "      <td id=\"T_abbe0_row12_col6\" class=\"data row12 col6\" >HY rich to IWM</td>\n",
       "      <td id=\"T_abbe0_row12_col7\" class=\"data row12 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row13_col0\" class=\"data row13 col0\" >HY</td>\n",
       "      <td id=\"T_abbe0_row13_col1\" class=\"data row13 col1\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row13_col2\" class=\"data row13 col2\" >HY Eqty</td>\n",
       "      <td id=\"T_abbe0_row13_col3\" class=\"data row13 col3\" >332.44</td>\n",
       "      <td id=\"T_abbe0_row13_col4\" class=\"data row13 col4\" >-0.41</td>\n",
       "      <td id=\"T_abbe0_row13_col5\" class=\"data row13 col5\" >-0.58</td>\n",
       "      <td id=\"T_abbe0_row13_col6\" class=\"data row13 col6\" >HY cheap to HY Eqty</td>\n",
       "      <td id=\"T_abbe0_row13_col7\" class=\"data row13 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row14_col0\" class=\"data row14 col0\" >MAIN</td>\n",
       "      <td id=\"T_abbe0_row14_col1\" class=\"data row14 col1\" >55.6113</td>\n",
       "      <td id=\"T_abbe0_row14_col2\" class=\"data row14 col2\" >SX5E</td>\n",
       "      <td id=\"T_abbe0_row14_col3\" class=\"data row14 col3\" >5670.81</td>\n",
       "      <td id=\"T_abbe0_row14_col4\" class=\"data row14 col4\" >0.21</td>\n",
       "      <td id=\"T_abbe0_row14_col5\" class=\"data row14 col5\" >-0.61</td>\n",
       "      <td id=\"T_abbe0_row14_col6\" class=\"data row14 col6\" >MAIN rich to SX5E</td>\n",
       "      <td id=\"T_abbe0_row14_col7\" class=\"data row14 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row15_col0\" class=\"data row15 col0\" >XOVER</td>\n",
       "      <td id=\"T_abbe0_row15_col1\" class=\"data row15 col1\" >269.0562</td>\n",
       "      <td id=\"T_abbe0_row15_col2\" class=\"data row15 col2\" >SX5E</td>\n",
       "      <td id=\"T_abbe0_row15_col3\" class=\"data row15 col3\" >5670.81</td>\n",
       "      <td id=\"T_abbe0_row15_col4\" class=\"data row15 col4\" >-0.15</td>\n",
       "      <td id=\"T_abbe0_row15_col5\" class=\"data row15 col5\" >-0.64</td>\n",
       "      <td id=\"T_abbe0_row15_col6\" class=\"data row15 col6\" >XOVER cheap to SX5E</td>\n",
       "      <td id=\"T_abbe0_row15_col7\" class=\"data row15 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row16_col0\" class=\"data row16 col0\" >MAIN</td>\n",
       "      <td id=\"T_abbe0_row16_col1\" class=\"data row16 col1\" >55.6113</td>\n",
       "      <td id=\"T_abbe0_row16_col2\" class=\"data row16 col2\" >IG</td>\n",
       "      <td id=\"T_abbe0_row16_col3\" class=\"data row16 col3\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row16_col4\" class=\"data row16 col4\" >0.35</td>\n",
       "      <td id=\"T_abbe0_row16_col5\" class=\"data row16 col5\" >0.49</td>\n",
       "      <td id=\"T_abbe0_row16_col6\" class=\"data row16 col6\" >MAIN rich to IG</td>\n",
       "      <td id=\"T_abbe0_row16_col7\" class=\"data row16 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row17_col0\" class=\"data row17 col0\" >XOVER</td>\n",
       "      <td id=\"T_abbe0_row17_col1\" class=\"data row17 col1\" >269.0562</td>\n",
       "      <td id=\"T_abbe0_row17_col2\" class=\"data row17 col2\" >HY</td>\n",
       "      <td id=\"T_abbe0_row17_col3\" class=\"data row17 col3\" >107.2074</td>\n",
       "      <td id=\"T_abbe0_row17_col4\" class=\"data row17 col4\" >0.23</td>\n",
       "      <td id=\"T_abbe0_row17_col5\" class=\"data row17 col5\" >0.01</td>\n",
       "      <td id=\"T_abbe0_row17_col6\" class=\"data row17 col6\" >XOVER rich to HY</td>\n",
       "      <td id=\"T_abbe0_row17_col7\" class=\"data row17 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row18_col0\" class=\"data row18 col0\" >EMB</td>\n",
       "      <td id=\"T_abbe0_row18_col1\" class=\"data row18 col1\" >96.03</td>\n",
       "      <td id=\"T_abbe0_row18_col2\" class=\"data row18 col2\" >EEM</td>\n",
       "      <td id=\"T_abbe0_row18_col3\" class=\"data row18 col3\" >54.655</td>\n",
       "      <td id=\"T_abbe0_row18_col4\" class=\"data row18 col4\" >0.59</td>\n",
       "      <td id=\"T_abbe0_row18_col5\" class=\"data row18 col5\" >-0.27</td>\n",
       "      <td id=\"T_abbe0_row18_col6\" class=\"data row18 col6\" >EMB rich to EEM</td>\n",
       "      <td id=\"T_abbe0_row18_col7\" class=\"data row18 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row19_col0\" class=\"data row19 col0\" >EM</td>\n",
       "      <td id=\"T_abbe0_row19_col1\" class=\"data row19 col1\" >97.74338</td>\n",
       "      <td id=\"T_abbe0_row19_col2\" class=\"data row19 col2\" >IG</td>\n",
       "      <td id=\"T_abbe0_row19_col3\" class=\"data row19 col3\" >52.5549</td>\n",
       "      <td id=\"T_abbe0_row19_col4\" class=\"data row19 col4\" >-0.25</td>\n",
       "      <td id=\"T_abbe0_row19_col5\" class=\"data row19 col5\" >-0.08</td>\n",
       "      <td id=\"T_abbe0_row19_col6\" class=\"data row19 col6\" >EM cheap to IG</td>\n",
       "      <td id=\"T_abbe0_row19_col7\" class=\"data row19 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row20_col0\" class=\"data row20 col0\" >EM</td>\n",
       "      <td id=\"T_abbe0_row20_col1\" class=\"data row20 col1\" >97.74338</td>\n",
       "      <td id=\"T_abbe0_row20_col2\" class=\"data row20 col2\" >XOVER</td>\n",
       "      <td id=\"T_abbe0_row20_col3\" class=\"data row20 col3\" >269.0562</td>\n",
       "      <td id=\"T_abbe0_row20_col4\" class=\"data row20 col4\" >0.05</td>\n",
       "      <td id=\"T_abbe0_row20_col5\" class=\"data row20 col5\" >-0.14</td>\n",
       "      <td id=\"T_abbe0_row20_col6\" class=\"data row20 col6\" >EM rich to XOVER</td>\n",
       "      <td id=\"T_abbe0_row20_col7\" class=\"data row20 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_abbe0_row21_col0\" class=\"data row21 col0\" >EM</td>\n",
       "      <td id=\"T_abbe0_row21_col1\" class=\"data row21 col1\" >97.74338</td>\n",
       "      <td id=\"T_abbe0_row21_col2\" class=\"data row21 col2\" >EEM</td>\n",
       "      <td id=\"T_abbe0_row21_col3\" class=\"data row21 col3\" >54.655</td>\n",
       "      <td id=\"T_abbe0_row21_col4\" class=\"data row21 col4\" >-0.14</td>\n",
       "      <td id=\"T_abbe0_row21_col5\" class=\"data row21 col5\" >-1.37</td>\n",
       "      <td id=\"T_abbe0_row21_col6\" class=\"data row21 col6\" >EM cheap to EEM</td>\n",
       "      <td id=\"T_abbe0_row21_col7\" class=\"data row21 col7\" >23-Oct 11:26 AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ed374574d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m     98\u001b[39m times_list += t1\n\u001b[32m    100\u001b[39m models_list = [ \n\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m     [\u001b[33m'\u001b[39m\u001b[33mCDX IG 5Y\u001b[39m\u001b[33m'\u001b[39m, [\u001b[33m'\u001b[39m\u001b[33mESA\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mCDX IG 5Y\u001b[39m\u001b[33m'\u001b[39m, [\u001b[33m'\u001b[39m\u001b[33mESA\u001b[39m\u001b[33m'\u001b[39m],],\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m \n\u001b[32m    122\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m l1, t1 = \u001b[43mx_model\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m l2 = pd.concat([l2,l1],axis=\u001b[32m1\u001b[39m)\n\u001b[32m    125\u001b[39m times_list += t1  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mx_model\u001b[39m\u001b[34m(model_num, models_list, diff_var, calc_method)\u001b[39m\n\u001b[32m    160\u001b[39m zscore_df[\u001b[33m'\u001b[39m\u001b[33mAvg. ZScore\u001b[39m\u001b[33m'\u001b[39m] = zscore_df[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m zscore_df.columns \u001b[38;5;28;01mif\u001b[39;00m col.endswith(\u001b[33m\"\u001b[39m\u001b[33m_ZScore\u001b[39m\u001b[33m\"\u001b[39m)]].mean(axis=\u001b[32m1\u001b[39m)\n\u001b[32m    161\u001b[39m zscore_df = zscore_df[[\u001b[33m'\u001b[39m\u001b[33mAvg. ZScore\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m bt_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheck_later\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_Y\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_X\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mzscore_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.dropna()\n\u001b[32m    164\u001b[39m bt_df = pd.concat([bt_df,beta],axis=\u001b[32m1\u001b[39m).dropna()\n\u001b[32m    166\u001b[39m zplot = bt_df[[\u001b[33m'\u001b[39m\u001b[33mAvg. ZScore\u001b[39m\u001b[33m'\u001b[39m]].copy()  \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    678\u001b[39m         obj_labels = obj.axes[\u001b[32m1\u001b[39m - ax]\n\u001b[32m    679\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels.equals(obj_labels):\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m             indexers[ax] = \u001b[43mobj_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m    684\u001b[39m new_data = concatenate_managers(\n\u001b[32m    685\u001b[39m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m.new_axes, concat_axis=\u001b[38;5;28mself\u001b[39m.bm_axis, copy=\u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m    686\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3953\u001b[39m, in \u001b[36mIndex.get_indexer\u001b[39m\u001b[34m(self, target, method, limit, tolerance)\u001b[39m\n\u001b[32m   3948\u001b[39m     target = target.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3949\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m this._get_indexer(\n\u001b[32m   3950\u001b[39m         target, method=method, limit=limit, tolerance=tolerance\n\u001b[32m   3951\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3953\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3980\u001b[39m, in \u001b[36mIndex._get_indexer\u001b[39m\u001b[34m(self, target, method, limit, tolerance)\u001b[39m\n\u001b[32m   3977\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3978\u001b[39m         tgt_values = target._get_engine_target()\n\u001b[32m-> \u001b[39m\u001b[32m3980\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ensure_platform_int(indexer)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "display(None, display_id=\"DFA\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ################################################################## Capture PX; check if market is live and attach\n",
    "    px = blp.bdp(tickers=all_bbg_tickers, flds='PX_LAST').T\n",
    "    bbg_time = datetime.now()\n",
    "    px.index= [f'{bbg_time}']\n",
    "    px.index = pd.to_datetime(px.index)\n",
    "    px = px.resample(\"1min\").last()\n",
    "    px.loc[px.index[0],'ESA INDEX'] = round(blp.bdp(tickers=\"ESA INDEX\", flds=\"PX_LAST\").iloc[0,0] * (spx_val/esa_val),2)\n",
    "    \n",
    "    if first_current_date_esa_close or bbg_time <= pd.to_datetime(\"09:33:00\"):\n",
    "        live_list = ['CDX IG CDSI GEN 5Y CORP','ITRX EUR CDSI GEN 5Y CORP', 'RSP US EQUITY','SX5E INDEX','ESA INDEX']\n",
    "        mkt_live = blp.bdh(tickers=live_list, flds='px_last', start_date = datetime.now()-timedelta(days=5))\n",
    "        mkt_live.columns = live_list\n",
    "        first_current_date_esa_close = False\n",
    "    \n",
    "    for col in px.columns:\n",
    "        if col == \"ESA INDEX\" and bbg_time.date() in mkt_live[live_dict[col]].dropna().index:\n",
    "            if bbg_time.time() >= pd.to_datetime(\"07:30\").time() and\\\n",
    "            bbg_time.time() <= (pd.to_datetime(\"20:00\") + timedelta(minutes=1)).time():\n",
    "                continue\n",
    "            else:\n",
    "                px.loc[px.index[0],col] = np.nan\n",
    "        elif bbg_time.date() in mkt_live[live_dict[col]].dropna().index and\\\n",
    "        bbg_time.time() >= pd.to_datetime(dict_map[reverse_dict[col]][1]).time() and\\\n",
    "        bbg_time.time() <= (pd.to_datetime(dict_map[reverse_dict[col]][2]) + timedelta(minutes=1)).time():\n",
    "            continue\n",
    "        else:\n",
    "            px.loc[px.index[0],col] = np.nan\n",
    "    px.columns = [reverse_dict[item] for item in px.columns]\n",
    "    bbg_datafile = pd.concat([bbg_datafile, px]).sort_index().copy()\n",
    "    bbg_datafile = bbg_datafile.resample(\"1min\").last().dropna(how='all').copy()\n",
    "\n",
    "    ################################################################## Convert PX to ER series and attach\n",
    "    current_er_series = None\n",
    "\n",
    "    for col in px.columns:\n",
    "        carry_days = (datetime.now() - pd.to_datetime(last_dict[col][2])).days\n",
    "        x = px[[col]].dropna().copy()\n",
    "\n",
    "        first_run = False\n",
    "        if col==\"ESA\":\n",
    "            first_run = True\n",
    "        elif dict_map[col][0] == \"Eq\":\n",
    "            first_run = True\n",
    "            \n",
    "        if first_run:\n",
    "            rate = funding[[item for item in funding.columns if col == item.split(\" \",2)[2].rsplit(\" \",1)[0]]].dropna().iloc[-1]\n",
    "            rate = rate.mean()   ############ etf funding rate\n",
    "            x = (last_dict[col][0])*((x/last_dict[col][1])-(rate/100)*(carry_days/360))\n",
    "        elif dict_map[col][4] == \"Yes\":\n",
    "            x = (last_dict[col][0])*(1 - dur[f'{col}_dq_dur'].iloc[-1]*(x-last_dict[col][1])*(10**(-4))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "        elif dict_map[col][0] == \"CDX\":\n",
    "            x = (last_dict[col][0])*(1 + (x-last_dict[col][1])*(10**(-2))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "        \n",
    "        current_er_series = pd.concat([current_er_series, x], axis=1)\n",
    "    all_intraday_er_series = pd.concat([all_intraday_er_series, current_er_series]).sort_index().copy()\n",
    "    all_intraday_er_series = all_intraday_er_series.resample(\"1min\").last().dropna(how='all').copy()\n",
    "    \n",
    "    last_update_time = all_intraday_er_series.copy()\n",
    "    \n",
    "    ##################################################################  Regressions starts from here\n",
    "    all_zscore_values = []\n",
    "\n",
    "    for diff_list in [[1,2,3],[6,9,12]]:        \n",
    "        times_list = []\n",
    "\n",
    "        if diff_list == [1,2,3]:\n",
    "            calc = \"Price\"\n",
    "        elif diff_list == [6,9,12]:\n",
    "            calc = \"Return\"            \n",
    "        \n",
    "        # model_Y, model_X (specify as a list) ### We trade these\n",
    "        # zscore_Y, zscore_X (specify as a list) ### We use these only for generating the zscore; names are taken from BBG datafile\n",
    "        \n",
    "        models_list = [\n",
    "            ['CDX IG 5Y', ['VCIT','IEF'], 'CDX IG 5Y', ['VCIT','IEF'],],\n",
    "        ]\n",
    "        l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "        l2 = l1.copy()\n",
    "        times_list += t1\n",
    "           \n",
    "        models_list = [ \n",
    "            ['CDX HY 5Y', ['HYG','IEI'], 'CDX HY 5Y', ['HYG','IEI'],],\n",
    "            \n",
    "        ]\n",
    "        l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "        l2 = pd.concat([l2,l1],axis=1)\n",
    "        times_list += t1\n",
    "        \n",
    "        models_list = [ \n",
    "            ['CDX EM 5Y', ['EMB','IEF'], 'CDX EM 5Y', ['EMB','IEF'],],\n",
    "        ]\n",
    "        l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "        l2 = pd.concat([l2,l1],axis=1)\n",
    "        times_list += t1\n",
    "        \n",
    "        models_list = [ \n",
    "        \n",
    "            ['CDX IG 5Y', ['ESA'], 'CDX IG 5Y', ['ESA'],],\n",
    "            ['CDX IG 5Y', ['SPY'], 'CDX IG 5Y', ['SPY'],],\n",
    "            ['CDX IG 5Y', ['RSP'], 'CDX IG 5Y', ['RSP'],],\n",
    "            ['CDX IG 5Y', ['IJH'], 'CDX IG 5Y', ['IJH'],],\n",
    "            ['CDX IG 5Y', ['IG Eqty'], 'CDX IG 5Y', ['IG Eqty'],],\n",
    "            \n",
    "            ['CDX HY 5Y', ['ESA'], 'CDX HY 5Y', ['ESA'],],\n",
    "            ['CDX HY 5Y', ['SPY'], 'CDX HY 5Y', ['SPY'],],\n",
    "            ['CDX HY 5Y', ['RSP'], 'CDX HY 5Y', ['RSP'],],\n",
    "            ['CDX HY 5Y', ['IJH'], 'CDX HY 5Y', ['IJH'],],\n",
    "            # ['CDX HY 5Y', ['RTY'], 'CDX HY 5Y', ['RTY'],],\n",
    "            ['CDX HY 5Y', ['IWM'], 'CDX HY 5Y', ['IWM'],],\n",
    "            ['CDX HY 5Y', ['HY Eqty'], 'CDX HY 5Y', ['HY Eqty'],],\n",
    "            \n",
    "            ['ITRX MAIN 5Y', ['SX5E'], 'ITRX MAIN 5Y', ['SX5E'], ],\n",
    "            ['ITRX XOVER 5Y', ['SX5E'], 'ITRX XOVER 5Y', ['SX5E'], ],\n",
    "            \n",
    "            ['ITRX MAIN 5Y', ['CDX IG 5Y'], 'ITRX MAIN 5Y', ['CDX IG 5Y'],],\n",
    "            ['ITRX XOVER 5Y', ['CDX HY 5Y'], 'ITRX XOVER 5Y', ['CDX HY 5Y'],],\n",
    "            \n",
    "        ]\n",
    "        l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "        l2 = pd.concat([l2,l1],axis=1)\n",
    "        times_list += t1  \n",
    "        \n",
    "        models_list = [ \n",
    "            ['EMB', ['EEM'], 'EMB', ['EEM'],],\n",
    "            ['CDX EM 5Y', ['CDX IG 5Y'], 'CDX EM 5Y', ['CDX IG 5Y'],],\n",
    "            ['CDX EM 5Y', ['ITRX XOVER 5Y'], 'CDX EM 5Y', ['ITRX XOVER 5Y'],],\n",
    "            ['CDX EM 5Y', ['EEM'], 'CDX EM 5Y', ['EEM'],],\n",
    "        ]\n",
    "        l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "        l2 = pd.concat([l2,l1],axis=1)\n",
    "        times_list += t1\n",
    "        all_zscore_values += [[item for item in l2.columns]]\n",
    "    \n",
    "    # ########################################################################################## Combining them\n",
    "    \n",
    "    df = pd.DataFrame({'PX_Zscore 1/2/3w':all_zscore_values[0]})\n",
    "    df['Rtn_Zscore 6/9/12w'] = [item.split(\": \")[1] for item in all_zscore_values[1]]\n",
    "    \n",
    "    df['Pair'] = df['PX_Zscore 1/2/3w'].apply(lambda x: x.split(\" ZScore: \")[0])\n",
    "    df['PX_Zscore 1/2/3w'] = df['PX_Zscore 1/2/3w'].apply(lambda x: x.split(\" ZScore: \")[1])\n",
    "    df['Pair'] = df['Pair'].str.replace(\"(\",\" (\")\n",
    "    df['Long Risk'] = df['Pair'].apply(lambda x: x.split(\" vs. \")[0])\n",
    "    df['Short Risk'] = df['Pair'].apply(lambda x: x.split(\" vs. \")[1])\n",
    "    df = df[['Long Risk','Short Risk','PX_Zscore 1/2/3w','Rtn_Zscore 6/9/12w']]\n",
    "    df['Long Ref.'] = df['Long Risk'].apply(lambda x: x.split(\" (\")[1].replace(\")\",\"\"))\n",
    "    df['Long Risk'] = df['Long Risk'].apply(lambda x: x.split(\" (\")[0].replace(\"CDX \",\"\").\\\n",
    "                                            replace(\"ITRX \",\"\").replace(\" 5Y\",\"\").replace(\"SPX\",\"ESA Implied SPY\"))\n",
    "    df['Short Ref.'] = df['Short Risk'].apply(lambda x: x.split(\" (\")[1].replace(\")\",\"\"))\n",
    "    df['Short Ref.'] = df['Short Ref.'].apply(lambda x: f\"['{x.split(\" & \")[0]}', '{x.split(\" & \")[1]}']\" if \"&\" in x else x)\n",
    "    df['Short Risk'] = df['Short Risk'].apply(lambda x: x.split(\" (\")[0])\n",
    "    df['Short Risk'] = df['Short Risk'].apply(lambda x: x.replace(\"ITRX \",\"\").\\\n",
    "                                              replace(\" 5Y\",\"\").replace(\"SPX\",\"ESA Implied SPY\").replace(\"CDX \",\"\"))\n",
    "    df['Valuation'] = df.apply(lambda row: f\"{row['Long Risk']} {'rich' if eval(row['PX_Zscore 1/2/3w'])\\\n",
    "    >0 else 'cheap'} to {row['Short Risk']}\",axis=1)\n",
    "   \n",
    "    df = df[['Long Risk','Long Ref.','Short Risk','Short Ref.','PX_Zscore 1/2/3w','Rtn_Zscore 6/9/12w','Valuation']].copy()\n",
    "    df['Last Update'] = [item.strftime(\"%d-%b %I:%M %p\") for item in times_list]\n",
    "    df = df.rename(columns={\"Long Ref.\":\"Ref\",\"Short Ref.\":\"Ref.\"})\n",
    "    \n",
    "    formatted_time = bbg_time.strftime(\"%I:%M %p\")\n",
    "    df.columns = pd.MultiIndex.from_tuples([(formatted_time, col) for col in df.columns])\n",
    "    \n",
    "    styled_df = (\n",
    "        df.style\n",
    "        .apply(color_negative_red_positive_green_basis, subset=[df.columns[4]], axis=0)\n",
    "        .apply(color_negative_red_positive_green_basis, subset=[df.columns[5]], axis=0)\n",
    "        .applymap(bold_zscore, subset=pd.IndexSlice[:, df.columns[4]])\n",
    "        .apply(add_black_line, axis=1)\n",
    "        .hide(axis=\"index\")\n",
    "    )\n",
    "    \n",
    "    update_display(styled_df,display_id=\"DFA\")\n",
    "    # bbg_datafile.to_parquet(\"bbg1.parquet\")\n",
    "    # all_intraday_er_series.to_parquet(\"er1.parquet\")\n",
    "    # if datetime.now().time()==pd.to_datetime(\"09:33\").time():\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
