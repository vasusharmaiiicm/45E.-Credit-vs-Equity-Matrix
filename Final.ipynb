{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e670e7-6fdc-4f86-888b-69ffbe02ca11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "from xbbg import blp\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, Dropdown, HBox, VBox, Button, Output, Text, widgets\n",
    "import sympy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, update_display\n",
    "from IPython import get_ipython\n",
    "# import backtrader.plot as btplot\n",
    "import matplotlib.dates as mdates\n",
    "from pydataquery import DataQuery\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import yfinance as yf\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##### Style df\n",
    "def bold_zscore(val):\n",
    "    return 'font-weight: bold' if val else ''\n",
    "\n",
    "def add_black_line(row):\n",
    "    return ['border-bottom: 3px solid black' if row.name in [2, 7, 13, 15, 17, 21] else '' for _ in row]\n",
    "\n",
    "def color_negative_red_positive_green_basis(col: pd.Series):\n",
    "    if col.empty:\n",
    "        return ['' for _ in col]\n",
    "\n",
    "    def safe_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    float_col = col.apply(safe_float)\n",
    "    min_val = float_col.min(skipna=True)\n",
    "    max_val = float_col.max(skipna=True)\n",
    "\n",
    "    def value_to_color(val):\n",
    "        if val is None:\n",
    "            return ''\n",
    "        if val < 0 and min_val < 0:\n",
    "            frac = val / min_val\n",
    "            frac = max(min(frac, 1), 0)\n",
    "            r = int(255 - (255 - 87) * frac)\n",
    "            g = int(255 - (255 - 187) * frac)\n",
    "            b = int(255 - (255 - 138) * frac)\n",
    "            return f'background-color: rgba({r},{g},{b},0.75)'\n",
    "        elif val > 0 and max_val > 0:\n",
    "            frac = val / max_val\n",
    "            frac = max(min(frac, 1), 0)\n",
    "            r = int(255 - (255 - 230) * frac)\n",
    "            g = int(255 - (255 - 135) * frac)\n",
    "            b = int(255 - (255 - 115) * frac)\n",
    "            return f'background-color: rgba({r},{g},{b},0.75)'\n",
    "        return ''\n",
    "\n",
    "    return [value_to_color(v) for v in float_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69dd64f-a300-4246-af64-5173def82c92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ################################## Updating betas of all pairs\n",
    "# all_start_date = '2020-01-01'#str((datetime.now()-timedelta(days=365*2.5)).date())\n",
    "\n",
    "# labels = {\n",
    "#         \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "#         # \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "#         \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "#         \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         # \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "#         \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         # \"CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX XOVER 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         # \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "#         \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "# }\n",
    "\n",
    "# dq = DataQuery(\n",
    "# client_id='jbAIMF2Tkp0JO3sc',\n",
    "# client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    "# )\n",
    "\n",
    "# job = dq.create_job(expressions = list(labels.values()))\n",
    "# dq.start_date = all_start_date\n",
    "# var = job.execute()\n",
    "# df = job.to_pivot_table()\n",
    "# df = df.T\n",
    "# df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "# df.index.name = 'Date'\n",
    "\n",
    "# df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "# df.columns.name = None\n",
    "# clear_output(wait=False)\n",
    "# df = df.dropna(how='all')\n",
    "# df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# dfx = df.copy()\n",
    "# dfx.index = pd.to_datetime(dfx.index)\n",
    "\n",
    "# end_date = dfx.index[-1]\n",
    "# df_pairs = dfx.copy()\n",
    "# dfx = dfx.resample('W').last()\n",
    "# master_dfx = (dfx.diff(1))/dfx.shift(1)\n",
    "\n",
    "# start_date = all_start_date\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity','IEAC LN Equity','IHYG LN EQUITY','BKLN US EQUITY']\n",
    "# securities = ['HYG US Equity','EMB US Equity','VCIT US Equity',]\n",
    "# fields1 = ['YAS_MOD_DUR']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = df.copy()\n",
    "\n",
    "# securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['HYG US Equity','EMB US Equity','VCIT US Equity',]\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['QW3I INDEX']\n",
    "# fields = ['MODIFIED_DURATION']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX']\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPX INDEX', 'RSP US EQUITY', 'RTY INDEX']\n",
    "# fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers=securities, start_date = start_date, end_date = end_date, flds = fields)\n",
    "# df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# df1.sort_index(inplace=True)\n",
    "# all_dq_initial_data = df1.copy()\n",
    "\n",
    "# ############################################### bbg_data\n",
    "# bbg_tick = ['HYG US Equity','EMB US Equity','VCIT US Equity',\\\n",
    "#             'LT03TRUU INDEX','LT09TRUU INDEX',\n",
    "#            'QW3I INDEX']\n",
    "\n",
    "# fields1 = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "# df = blp.bdh(tickers = bbg_tick, start_date = start_date, end_date = end_date, flds = fields1)\n",
    "# df.columns = ['TR ' + list(item)[0].split(' ')[0] for item in list(df.columns)]\n",
    "# all_bbg_data_initial = df.copy()\n",
    "\n",
    "# ################################################################\n",
    "\n",
    "# df_beta = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "# df_beta['Date'] = pd.to_datetime(df_beta['Date'])\n",
    "# df_beta = df_beta.set_index('Date')\n",
    "# df_beta = df_beta[df_beta.index<=datetime.now()]\n",
    "\n",
    "# all_beta_df = None\n",
    "# backtest_last_date = df_beta.index[-1].date() - timedelta(days=8)\n",
    "\n",
    "# while backtest_last_date <= datetime.now().date():\n",
    "#     try:\n",
    "#         dfx = master_dfx.copy()\n",
    "#         # all_start_date = str((datetime.now()-timedelta(days=365*2.5)).date())\n",
    "        \n",
    "#         tickers = {\n",
    "#             tuple(['ER CDX HY 5Y','TR HYG - (HYG DUR/LT03TRUU DUR) * TR LT03TRUU']): [1,1],\n",
    "#             tuple(['ER CDX EM 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX HY 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 5Y','TR EMB - (EMB DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 10Y','TR LQD - (LQD DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             tuple(['ER CDX IG 5Y','TR VCIT - (VCIT DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "#             # tuple(['ER CDX IG 10Y','TR VCIT - (VCIT DUR/LT09TRUU DUR) * TR LT09TRUU']): [1,1],\n",
    "            \n",
    "#             # tuple(['ER ITRX MAIN 5Y','TR IEAC - (IEAC DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#             # tuple(['ER ITRX XOVER 5Y','TR IHYG - (IHYG DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#             # tuple(['ER CDX HY 5Y','TR IHYG - (IHYG DUR/QW3I DUR) * TR QW3I']): [1,1],\n",
    "#         }\n",
    "        \n",
    "#         ratios = list(tickers.values())\n",
    "#         ratios = [list(item) for item in ratios]\n",
    "        \n",
    "#         tick = list(tickers.keys())\n",
    "#         tick = [list(item) for item in tick]\n",
    "        \n",
    "#         bbg_data_initial = all_bbg_data_initial[all_bbg_data_initial.index<=pd.to_datetime(backtest_last_date).date()].copy()\n",
    "#         dq_initial_data = all_dq_initial_data[all_dq_initial_data.index<=pd.to_datetime(backtest_last_date).date()].copy()\n",
    "    \n",
    "#         df2 = dq_initial_data.copy()\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "        \n",
    "#         dfA = df2[[col for col in df2.columns if (col.startswith(\"TR\") or col.startswith(\"ER\"))]]\n",
    "#         dfA = dfA.resample('W').last()\n",
    "#         dfA = dfA.diff(1)/dfA.shift(1)\n",
    "#         dfB = df2[[col for col in df2.columns if col.endswith(\"DUR\")]]\n",
    "#         df_dur = dfB.copy()\n",
    "#         dfB = dfB.resample('W').mean()\n",
    "#         df2 = pd.concat([dfA, dfB], axis=1)\n",
    "        \n",
    "#         df3 = df2.copy()\n",
    "#         for i in range(len(tickers)):\n",
    "#             item = list(list(tickers.keys())[i])[1]\n",
    "#             if ' - ' in item:\n",
    "#                 first = item.split(' - ')[0]\n",
    "#                 second = item.split(' - ')[1].split('/')[0][1:]\n",
    "#                 third = item.split(' - ')[1].split('/')[1].split(\")\")[0]\n",
    "#                 fourth = item.split(' - ')[1].split('/')[1].split(\"* \")[1]\n",
    "#                 df3[item] = df3[first] - (df3[second]/df3[third]) * df3[fourth]\n",
    "#         df3 = df3[[col for col in df3.columns if ' - ' in col]]\n",
    "        \n",
    "#         dfx.index = pd.to_datetime(dfx.index)\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "#         df3.index = pd.to_datetime(df3.index)\n",
    "#         df3 = pd.concat([dfx,df2,df3],axis=1)\n",
    "        \n",
    "#         df3 = df3.iloc[:,~df3.columns.duplicated()]\n",
    "        \n",
    "#         df4 = df3.copy()\n",
    "#         df_deming = df4.copy()\n",
    "        \n",
    "#         # df4 = df4[df4.index.year != 2020]\n",
    "#         # df4 = df4[df4.index > pd.to_datetime('2021-01-01')]\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             df5 = df4[tick[i]].copy()\n",
    "#             for j in range(len(df5)-24):\n",
    "#                 Y = df5[tick[i]].iloc[j:j+24].dropna()[tick[i][0]] * 1#ratios[i][0]\n",
    "#                 X = df5[tick[i]].iloc[j:j+24].dropna()[tick[i][1]] * 1#ratios[i][1]\n",
    "#                 if ((len(Y) > 20) and (len(X) == len(Y))):\n",
    "#                     X = sm.add_constant(X)\n",
    "#                     model = sm.OLS(Y,X).fit()\n",
    "#                     df4.loc[f'{df5.index[j+24]}',f'{tick[i][0]} vs. {tick[i][1]}'] = model.params.iloc[1]\n",
    "        \n",
    "#         df5 = df4[[col for col in df4.columns if 'vs.' in col]].copy()\n",
    "#         df2.index = pd.to_datetime(df2.index)\n",
    "        \n",
    "#         df5 = df5.dropna(how='all')\n",
    "        \n",
    "#         df6 = df5.rolling(window=104).mean().iloc[[-1]].copy() #52 weeks in a year => 104 weeks in 2 years\n",
    "        \n",
    "#         df6 = df6[[col for col in df6.columns if ' - ' in col]]\n",
    "        \n",
    "#         l1 = [item.split(\" vs. \")[0] for item in df6.columns]\n",
    "#         l2 = [item.split(\" vs. \")[1].split(' - ',1)[0] for item in df6.columns]\n",
    "#         l3 = [item.split(\" vs. \")[1].split(' - ',1)[1].split(\" * \")[1] for item in df6.columns]\n",
    "#         l = []\n",
    "        \n",
    "#         for i in range(len(l1)):\n",
    "#             l += [tuple([l1[i]] + [l2[i]] + [l3[i]])]\n",
    "        \n",
    "#         m = list(df6.iloc[-1])\n",
    "#         m = [[1,-1*item] for item in m]\n",
    "        \n",
    "#         tickers_2 = dict(zip(l,m))\n",
    "        \n",
    "#         ratios = list(tickers_2.values())\n",
    "#         ratios = [list(item) for item in ratios]\n",
    "        \n",
    "#         tick = list(tickers_2.keys())\n",
    "#         tick = [list(item) for item in tick]\n",
    "        \n",
    "        \n",
    "#         df = bbg_data_initial.copy()\n",
    "#         df.index = pd.to_datetime(df.index)\n",
    "#         df = df.resample('W').last()\n",
    "#         df = df.diff()/df.shift(1)\n",
    "#         df2 = pd.concat([dfx,df],axis=1)\n",
    "#         df2 = df2.dropna()\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             df2[f'{ratios[i][0]}*{tick[i][0]} + ({ratios[i][1]})*{tick[i][1]}'] = (ratios[i][0] * df2[tick[i][0]] + ratios[i][1] * df2[tick[i][1]])\n",
    "        \n",
    "#         df4 = df2.copy()\n",
    "#         df4.index = pd.to_datetime(df4.index)\n",
    "        \n",
    "#         # df4 = df4[df4.index > pd.to_datetime('2021-01-01')]\n",
    "        \n",
    "#         for i in range(len(tick)):\n",
    "#             for col in df4.columns:\n",
    "#                 if ((tick[i][0] in col) and (tick[i][1] in col)):\n",
    "#                     globals()[f'df_{i}'] = df4[[col,tick[i][2]]].dropna().copy()\n",
    "#                     val = []\n",
    "#                     for z in range(len(globals()[f'df_{i}'])-24):\n",
    "#                         Y = globals()[f'df_{i}'].iloc[z:z+24,0]\n",
    "#                         X = globals()[f'df_{i}'].iloc[z:z+24,1]\n",
    "#                         X = sm.add_constant(X)\n",
    "#                         model = sm.OLS(Y,X).fit()\n",
    "#                         val.append(model.params.iloc[1])\n",
    "#                     val1 = [np.nan] * 24 + val\n",
    "#                     globals()[f'df_{i}'][f'{list(globals()[f'df_{i}'].columns)}'] = val1\n",
    "        \n",
    "        \n",
    "        \n",
    "#         periods = {'6M':0.5, '1Y':1, '2Y':2, '3Y':3, '4Y':4}\n",
    "#         periods = { '2Y':2 }\n",
    "        \n",
    "#         num = list(periods.values())\n",
    "#         name = list(periods.keys())\n",
    "        \n",
    "#         dfy = pd.DataFrame()\n",
    "        \n",
    "#         for i in range(len(periods)):\n",
    "#             l = df5.rolling(window=int(52*num[i])).mean().iloc[[-1]].T\n",
    "#             dfy = pd.concat([dfy,l],axis=1)\n",
    "        \n",
    "#         dfy.columns = list(periods.keys())\n",
    "        \n",
    "#         dfz = pd.DataFrame()\n",
    "#         for i in range(len(tick)):\n",
    "#             dfz = pd.concat([dfz, globals()[f'df_{i}'].iloc[:,[-1]]],axis=1)\n",
    "#         dfz = dfz.sort_index()\n",
    "#         dfy1 = pd.DataFrame()\n",
    "        \n",
    "#         for i in range(len(periods)):\n",
    "#             l = dfz.rolling(window=int(52*num[i])).mean().iloc[[-1]].T\n",
    "#             l.columns = [name[i]]\n",
    "#             dfy1 = pd.concat([dfy1,l],axis=1)\n",
    "        \n",
    "#         dfA = dfy.copy()\n",
    "#         dfB = dfy1.copy()\n",
    "        \n",
    "#         dfy = dfy.astype(float).round(2)\n",
    "#         dfy1 = dfy1.astype(float).round(2)\n",
    "        \n",
    "#         dfy = dfy.astype(str)\n",
    "#         dfy1 = dfy1.astype(str)\n",
    "        \n",
    "#         rows = len(dfy1)\n",
    "#         columns = len(dfy1.columns)\n",
    "#         for i in range(rows):\n",
    "#             for j in range(columns):\n",
    "#                 st = '[' + str(dfy.iloc[i,j]) + ', '+ str(dfy1.iloc[i,j]) + ']'\n",
    "#                 dfy.iloc[i,j] = st\n",
    "                \n",
    "#         x21 = dfy[['2Y']].T.copy()\n",
    "#         x21.index.name = 'Date'\n",
    "#         x21.columns.name = ''\n",
    "#         x21.index = [backtest_last_date]\n",
    "\n",
    "#         backtest_last_date += timedelta(days=1)\n",
    "#     except Exception as e:\n",
    "#         x21 = None\n",
    "\n",
    "#     all_beta_df = pd.concat([all_beta_df,x21])\n",
    "\n",
    "# all_beta_df.index = pd.to_datetime(all_beta_df.index)\n",
    "# col_list = list(all_beta_df.columns)\n",
    "# col_list = [item.replace(\"ER \",\"\").replace(\" TR \",\"\").replace(\" vs.\",\"_\")\\\n",
    "#             .replace(\"LT03TRUU\",\"IEI\").replace(\"LT09TRUU\",\"IEF\")for item in col_list]\n",
    "# col_list1 = [item.split(\" - \")[0]+'_'+item.split(\" *\")[1] for item in col_list]\n",
    "# all_beta_df1 = all_beta_df.copy()\n",
    "\n",
    "# all_beta_df1.columns = col_list1\n",
    "# x = pd.concat([all_beta_df1,df_beta])\n",
    "# x = x.sort_index()\n",
    "# x = x[~x.index.duplicated(keep='last')]\n",
    "# x = x.dropna(axis=1)\n",
    "# last_beta = x.index[-1]\n",
    "# x.loc[last_beta+timedelta(days=1)] = x.iloc[-1,:].to_list()\n",
    "# x.loc[last_beta+timedelta(days=2)] = x.iloc[-1,:].to_list()\n",
    "# x.index.name = 'Date'\n",
    "# x.to_csv(\"All Basis Trade Betas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94fccbf4-98ba-424f-90ae-ca08b44f4161",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ER Code\n",
    "####################################################\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "        \"LQD Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDLIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"HYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_USDHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"IEAC Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURIG_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"IHYG Funding Sprd\": \"DB(CDS,TRAC-X,EUROPEIBXTRS_EURHY_3M,JPM_IMPLIEDFUNDING_MID)\",\n",
    "        \"Fed Fund\": \"FF\",\n",
    "        \"ER CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX HY 10Y\": \"DB(CDS,TRAC-X,NAHY100UNF10ONRUN,JPM_RETURN)\",\n",
    "        \"ER CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_RETURN)\",\n",
    "        \"ER ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "        \"ER ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_UNFUNDED_INDEX)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "dq = df.copy()\n",
    "\n",
    "# dq['Fed Fund'] = dq['Fed Fund'].ffill()\n",
    "\n",
    "end_date = dq.index[-1]\n",
    "####################################### BBG Data Acquisition\n",
    "\n",
    "securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "              'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY', 'IBCN GR EQUITY',\n",
    "              'IEI US Equity','IEF US Equity']\n",
    "\n",
    "fields1 = ['YAS_MOD_DUR']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields1)\n",
    "df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "df1 = df.copy()\n",
    "\n",
    "#################################### Fixing Bad Data Point in YAS of IEI\n",
    "rolling_avg = df1['IEI DUR'].replace(0, np.nan).rolling(window=30, min_periods=1).mean()\n",
    "df1['IEI DUR'] = df1.apply(\n",
    "    lambda row: rolling_avg[row.name] if row['IEI DUR'] == 0.0 else row['IEI DUR'], axis=1\n",
    ")\n",
    "#################################### Fixing Bad Data Point in YAS of IEI\n",
    "\n",
    "securities = ['LT03TRUU INDEX','LT09TRUU INDEX','QW3I INDEX', 'LT03MD INDEX','LT09MD INDEX']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['TR ' + item.split(' ')[0] for item in securities[:3]] + [item.split(' ')[0] + ' DUR' for item in securities[:2]]\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['HYG US Equity','EMB US Equity','LQD US Equity','VCIT US Equity',\n",
    "              'IEI US Equity','IEF US Equity', 'RSP US EQUITY', #'SPX INDEX',  'RTY INDEX',\n",
    "              'IBCN GR EQUITY',\n",
    "              'IEAC LN Equity','IHYG LN EQUITY', 'BKLN US EQUITY',\n",
    "              'GSCBHYEQ Index', 'GSCBIGEQ Index', 'SPY US EQUITY', 'EEM US EQUITY', 'IWM US EQUITY', 'IJH US EQUITY',\n",
    "             ]\n",
    "\n",
    "fields = ['TOT_RETURN_INDEX_GROSS_DVDS']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['TR ' + item.split(' ')[0] for item in securities] \n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['QW3I INDEX']\n",
    "fields = ['MODIFIED_DURATION']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = [item.split(' ')[0] + ' DUR' for item in securities]\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "# securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']  ############## I want to calculate funding rate for spx, rty and sx5e separately\n",
    "# fields = ['PX_LAST']\n",
    "# df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "# df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "# df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "securities = ['EURR002W Index']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = all_start_date, flds = fields)\n",
    "df.columns = ['ECB Rate']\n",
    "df1 = pd.concat([df,df1],axis=1)\n",
    "\n",
    "bbg = df1.copy()\n",
    "dq.index = pd.to_datetime(dq.index)\n",
    "dq.index = dq.index.date\n",
    "bbg.index = pd.to_datetime(bbg.index)\n",
    "bbg.index = bbg.index.date\n",
    "\n",
    "data = pd.concat([dq,bbg],axis=1)\n",
    "data = data.sort_index()\n",
    "\n",
    "df_funding = data[[col for col in data.columns if ('Funding Sprd' in col)]+['Fed Fund']+['ECB Rate']]\n",
    "\n",
    "if np.isnan(df_funding.loc[df_funding.index[-1],'Fed Fund']):\n",
    "    df_funding.loc[df_funding.index[-1],'Fed Fund'] = df_funding.loc[df_funding.index[-2],'Fed Fund']\n",
    "\n",
    "# df_funding['Fed Fund'] = df_funding['Fed Fund'].ffill()\n",
    "\n",
    "for col in df_funding:\n",
    "    if col.endswith('Sprd'):\n",
    "        if col.split(' ')[0] in ['HYG','LQD']:\n",
    "            df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "            df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['Fed Fund'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "        if col.split(' ')[0] in ['IHYG','IEAC']:\n",
    "            df_funding[f'Net Long {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) + 0.25/100\n",
    "            df_funding[f'Net Short {col.replace(\" Sprd\",\"\")}'] = (df_funding['ECB Rate'] + df_funding[f'{col}']/100) - 0.25/100\n",
    "\n",
    "df_funding['Net Long VCIT Funding'] = df_funding['Net Long LQD Funding']\n",
    "df_funding['Net Short VCIT Funding'] = df_funding['Net Short LQD Funding']\n",
    "\n",
    "for item in ['EMB','EEM']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.5\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.5\n",
    "\n",
    "for item in ['IEI', 'IEF', 'RSP', 'BKLN', 'GSCBHYEQ', 'GSCBIGEQ', 'SPX', 'RTY', 'SPY', 'IWM', 'IJH']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['Fed Fund'] + 0.15\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['Fed Fund'] - 0.15\n",
    "\n",
    "for item in ['IBCN','SX5E']:\n",
    "    df_funding[f'Net Long {item} Funding'] = df_funding['ECB Rate'] + 0.15\n",
    "    df_funding[f'Net Short {item} Funding'] = df_funding['ECB Rate'] - 0.15\n",
    "\n",
    "df_funding = df_funding[[col for col in df_funding.columns if col.startswith(\"Net\")]]\n",
    "df_funding.index = pd.to_datetime(df_funding.index)\n",
    "df_funding = df_funding.resample('D').last().ffill()\n",
    "\n",
    "original_er_data = data[[col for col in data.columns if col.startswith(\"ER \")]]\n",
    "tr_data = data[[col for col in data.columns if col.startswith(\"TR \")]]\n",
    "ust = tr_data[['TR LT09TRUU']] # for using corr later\n",
    "tr_data = tr_data.iloc[:,:-3] #dropping LT03/09 and QW3I\n",
    "\n",
    "tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "er_tr_data = er_tr_data.sort_index()\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "for item in etfs:\n",
    "    check = er_tr_data[item].dropna()\n",
    "    check = check.diff()/check.shift()\n",
    "    check = check.reindex(er_tr_data.index)\n",
    "    er_tr_data[item] = check\n",
    "    \n",
    "er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "############################################################### Funding Sprds\n",
    "funding = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "x = er_tr_data[[col for col in er_tr_data.columns if 'Funding' in col]].copy()\n",
    "x = x.interpolate()\n",
    "x.to_excel(\"Funding Rates.xlsx\")\n",
    "\n",
    "y = x.copy()\n",
    "y = round(y,2)\n",
    "y.to_excel(\"Funding Rates 2.xlsx\")\n",
    "\n",
    "###############################################################\n",
    "for item in etfs:\n",
    "    name = item.split(' ')[1]\n",
    "    er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "                (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "\n",
    "er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "tr_data.index = pd.to_datetime(tr_data.index).date\n",
    "df_funding.index = pd.to_datetime(df_funding.index).date\n",
    "\n",
    "er_tr_data = pd.concat([tr_data,df_funding],axis=1)\n",
    "er_tr_data = er_tr_data.sort_index()\n",
    "\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "etfs = [col for col in er_tr_data.columns if col.startswith(\"TR \")]\n",
    "\n",
    "for item in etfs:\n",
    "    check = er_tr_data[item].dropna()\n",
    "    check = check.diff()/check.shift()\n",
    "    check = check.reindex(er_tr_data.index)\n",
    "    er_tr_data[item] = check\n",
    "\n",
    "er_tr_data['Date'] = pd.to_datetime(er_tr_data.index)\n",
    "er_tr_data['Days'] = (er_tr_data['Date'] - er_tr_data['Date'].shift()).dt.days\n",
    "# er_tr_data = er_tr_data.dropna()\n",
    "\n",
    "for item in etfs:\n",
    "    name = item.split(' ')[1]\n",
    "    er_tr_data[f'ER {name}'] = er_tr_data[item] - \\\n",
    "                (1/100)*(er_tr_data['Days']/360)*(0.5*(er_tr_data[f'Net Long {name} Funding'] + er_tr_data[f'Net Short {name} Funding']))\n",
    "\n",
    "er_tr_data = er_tr_data[[col for col in er_tr_data.columns if col.startswith(\"ER \")]]\n",
    "er_tr_data = (1+er_tr_data).cumprod()\n",
    "\n",
    "er_data = pd.concat([original_er_data,er_tr_data],axis=1)\n",
    "# er_data = er_data.dropna()\n",
    "# er_data.columns = er_data.columns.str.replace(\"ER SPX\",\"ER ESA\").str.replace(\"ER RTY\",\"ER RTYA\").str.replace(\"ER SX5E\",\"ER VGA\")\n",
    "er_data.columns = er_data.columns.str.replace(\"ER GSCBHYEQ\",\"ER HY Eqty\").str.replace(\"ER GSCBIGEQ\",\"ER IG Eqty\")\n",
    "er_data = er_data.sort_index()\n",
    "\n",
    "securities = ['SPXFP INDEX', 'RTYFPE INDEX','SX5EFSER Index']\n",
    "fields = ['PX_LAST']\n",
    "df = blp.bdh(tickers=securities, start_date = er_data.index[0], flds = fields)\n",
    "df.columns = ['ER SPX','ER RTY','ER SX5E']\n",
    "er_data = pd.concat([er_data,df], axis=1)\n",
    "er_data = er_data.sort_index()\n",
    "\n",
    "er_data.to_csv(\"All ER.csv\")\n",
    "\n",
    "##############################################################  Updating Durations\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "        \"CDX IG 5Y Dur\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "        \"CDX IG 10Y Dur\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "        \"ITRX XOVER 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX MAIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX MAIN 10Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "        \"ITRX SUBFIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "        \"ITRX SNRFIN 5Y Dur\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.loc[pd.to_datetime(datetime.now().date())] = [item - ((datetime.now() - df.index[-1]).days)/365 for item in list(df.iloc[-1])]\n",
    "df = df.interpolate()\n",
    "all_dq_dur = df.copy()\n",
    "all_dq_dur.to_excel(\"All DQ Duration.xlsx\")\n",
    "\n",
    "############################################################################## Updating Ref levels\n",
    "\n",
    "all_start_date = str((datetime.now()-timedelta(days=6*365+1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "labels = {\n",
    "    \"CDX IG 5Y\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX IG 10Y\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX HY 5Y\": \"DB(CDS,TRAC-X,NAHY100UNF05ONRUN,JPM_CLEAN_MID)\",\n",
    "    \"ITRX MAIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX MAIN 10Y\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX SNRFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX SUBFIN 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"ITRX XOVER 5Y\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_CDSSPREAD_MID)\",\n",
    "    \"CDX EM 5Y\": \"DB(NEO-UK,credit/cds/index/cdxEM-onrun/5y/JPM_CLEAN_MID)\",\n",
    "\n",
    "    \"CDX IG 5Y DUR\": \"DB(CDS,TRAC-X,NAMERI100UNF05ONRUN,JPM_DUR)\",\n",
    "    \"CDX IG 10Y DUR\": \"DB(CDS,TRAC-X,NAMERI100UNF10ONRUN,JPM_DUR)\",\n",
    "    \"ITRX MAIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX MAIN 10Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx-onrun/10y/JPM_DUR)\",\n",
    "    \"ITRX SNRFIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_senfin-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX SUBFIN 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_subfin-onrun/5y/JPM_DUR)\",\n",
    "    \"ITRX XOVER 5Y DUR\": \"DB(NEO-UK,credit/cds/index/itraxx_crossover-onrun/5y/JPM_DUR)\",\n",
    "}\n",
    "\n",
    "dq = DataQuery(\n",
    "client_id='jbAIMF2Tkp0JO3sc',\n",
    "client_secret='d7qfzgt55pddjs352sgxosFyI4t2eja07k7opbi6wg9oqjc1OjkdAksn1btmnugeMjchcx2vwTsJupw',\n",
    ")\n",
    "\n",
    "job = dq.create_job(expressions = list(labels.values()))\n",
    "dq.start_date = all_start_date\n",
    "var = job.execute()\n",
    "df = job.to_pivot_table()\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index, format='%Y%m%d').date\n",
    "df.index.name = 'Date'\n",
    "\n",
    "df.rename(columns={v:k for k, v in labels.items()},inplace=True)\n",
    "df.columns.name = None\n",
    "clear_output(wait=False)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.to_excel(\"Ref Levels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b0c5ad-9596-4892-8828-c7a78fc078e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_dq_dur = pd.read_excel(\"All DQ Duration.xlsx\")\n",
    "all_dq_dur['Date'] = pd.to_datetime(all_dq_dur['Date'])\n",
    "all_dq_dur = all_dq_dur.set_index('Date')\n",
    "\n",
    "dict_map = {\n",
    "# product type, start time, end time, carry (%), trades on sprd, slippage (bps or $),\n",
    "# fixed commission, notional (if selected as Y), BBG ticker, check live status using which ticker\n",
    "    'CDX IG 5Y': ['CDX', '07:30:00', '20:00:00', 1, 'Yes', 0.15, 500, 30*10**6, \"CDX IG CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX IG 10Y': ['CDX', '07:30:00', '20:00:00', 1, 'Yes', 0.3, 500, 30*10**6, \"CDX IG CDSI GEN 10Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'CDX HY 5Y': ['CDX', '07:30:00', '20:00:00', 5, 'No', 0.02, 500, 6*10**6, \"CDX HY CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'SPX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"SPX INDEX\", \"ESA INDEX\"],\n",
    "    'SPY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"SPY US EQUITY\", \"ESA INDEX\"],\n",
    "    'IWM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IWM US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'RSP': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"RSP US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'RTY': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"RTY INDEX\", \"RSP US EQUITY\"],\n",
    "    'IG Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"GSCBIGEQ Index\", \"RSP US EQUITY\"],\n",
    "    'HY Eqty': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"GSCBHYEQ Index\", \"RSP US EQUITY\"],\n",
    "    'ITRX MAIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"ITRX EUR CDSI GEN 5Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX MAIN 10Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.3, 500, 30*10**6, \"ITRX EUR CDSI GEN 10Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX SNRFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"SNRFIN CDSI GEN 5Y Corp\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX SUBFIN 5Y': ['CDX', '03:30:00', '11:59:00', 1, 'Yes', 0.15, 500, 30*10**6, \"SUBFIN CDSI GEN 5Y Corp\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 5Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.15, 500, 6*10**6, \"ITRX XOVER CDSI GEN 5Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    'ITRX XOVER 10Y': ['CDX', '03:30:00', '11:59:00', 5, 'Yes', 0.3, 500, 6*10**6, \"ITRX XOVER CDSI GEN 10Y CORP\", \"ITRX EUR CDSI GEN 5Y CORP\"],\n",
    "    # 'VIX': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"VIX INDEX\", \"RSP US EQUITY\"],\n",
    "    # 'V2X': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"V2X INDEX\", \"SX5E INDEX\"],\n",
    "    'SX5E': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"SX5E INDEX\", \"SX5E INDEX\"],\n",
    "    'CDX EM 5Y': ['CDX', '07:30:00', '20:00:00', 1, 'No', 0.02, 500, 6*10**6, \"CDX EM CDSI GEN 5Y CORP\", \"CDX IG CDSI GEN 5Y CORP\"],\n",
    "    'HYG': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"HYG US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'EMB': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"EMB US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'VCIT': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"VCIT US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'LQD': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"LQD US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IEI': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IEI US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IEF': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IEF US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'EEM': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"EEM US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IJH': ['Eq', '09:30:00', '15:59:00', 0, 'No', 0.01, 0, 10**6, \"IJH US EQUITY\", \"RSP US EQUITY\"],\n",
    "    'IEAC': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"IEAC LN EQUITY\", \"SX5E INDEX\"],\n",
    "    'IHYG': ['Eq', '03:30:00', '11:59:00', 0, 'No', 0.01, 0, 10**6, \"IHYG LN EQUITY\", \"SX5E INDEX\"],\n",
    "}\n",
    "\n",
    "l1 = list(dict_map.keys())\n",
    "l2 = [item[8] for item in list(dict_map.values())]\n",
    "reverse_dict = dict(zip(l2,l1))\n",
    "reverse_dict[\"ESA INDEX\"] = \"ESA\"\n",
    "\n",
    "live_dict = dict(zip([item[8] for item in dict_map.values()], [item[9] for item in dict_map.values()]))\n",
    "live_dict[\"ESA INDEX\"] = \"ESA INDEX\"\n",
    "\n",
    "last_checked = None\n",
    "current_date_esa_close = datetime.now()\n",
    "first_current_date_esa_close = True\n",
    "first_run = True\n",
    "\n",
    "df = pd.read_excel(\"Data for Credit-Eqty Dashboard v4.xlsx\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')\n",
    "df = df.sort_index()\n",
    "bbg_datafile = df.copy()\n",
    "last_update = df.dropna().index[-1]\n",
    "all_bbg_tickers = [dict_map[item][8] for item in df.columns]\n",
    "new_data = pd.DataFrame()\n",
    "\n",
    "for bbg_tickers in all_bbg_tickers:\n",
    "    bbg_date = pd.to_datetime(last_update.date())\n",
    "    data = None\n",
    "    while True:\n",
    "        try:\n",
    "            f = blp.bdib(ticker = bbg_tickers, dt = bbg_date, interval = 1, ref='IndexYieldCurve')      \n",
    "        except:\n",
    "            if bbg_date > pd.to_datetime(datetime.now().date()):\n",
    "                break\n",
    "        data = pd.concat([data,f])\n",
    "        bbg_date += timedelta(days=1)    \n",
    "    try:\n",
    "        data = data.iloc[:,[3]].copy()\n",
    "        data.columns = [bbg_tickers]\n",
    "    except:\n",
    "        continue\n",
    "    new_data = pd.concat([new_data, data],axis=1)\n",
    "\n",
    "new_data.index = new_data.index.tz_convert('America/New_York')\n",
    "new_data.index = new_data.index.tz_localize(None)\n",
    "\n",
    "live_list = ['CDX IG CDSI GEN 5Y CORP','ITRX EUR CDSI GEN 5Y CORP', 'RSP US EQUITY','SX5E INDEX','ESA INDEX']\n",
    "mask = blp.bdh(tickers = live_list, flds='PX_LAST', start_date = df.index[0]-timedelta(days=5))\n",
    "mask.columns = live_list\n",
    "mask.index = pd.to_datetime(mask.index).date\n",
    "\n",
    "new_data = new_data.ffill()\n",
    "new_data.index = pd.to_datetime(new_data.index)\n",
    "new_data.columns = [reverse_dict[item] for item in new_data.columns]\n",
    "\n",
    "bbg_datafile = bbg_datafile[bbg_datafile.index<new_data.index[0]].copy()\n",
    "bbg_datafile = pd.concat([bbg_datafile, new_data])\n",
    "bbg_datafile = bbg_datafile.sort_index()\n",
    "bbg_datafile.index.name = 'Date'\n",
    "bbg_datafile = bbg_datafile.ffill().copy()\n",
    "bbg_datafile = bbg_datafile[~bbg_datafile.index.duplicated(keep='last')]\n",
    "\n",
    "bbg_datafile['Date'] = pd.to_datetime(bbg_datafile.index.date)\n",
    "bbg_datafile['Time'] = bbg_datafile.index.time\n",
    "filtered_new_data = None\n",
    "\n",
    "for col in bbg_datafile.drop(['Date','Time'],axis=1).columns:\n",
    "    test = bbg_datafile[[col,'Date','Time']]\n",
    "    cond = (test['Date'].isin(mask[dict_map[col][9]].dropna().index)) & (test['Time']<=pd.\\\n",
    "            to_datetime(dict_map[col][2]).time()) & (test['Time']>=pd.to_datetime(dict_map[col][1]).time())\n",
    "    test = test[cond][[col]]\n",
    "    filtered_new_data = pd.concat([filtered_new_data, test],axis=1)\n",
    "\n",
    "bbg_datafile = filtered_new_data.copy()\n",
    "\n",
    "bbg_datafile = bbg_datafile.dropna(how='all')\n",
    "bbg_datafile.to_excel(\"Data for Credit-Eqty Dashboard v4.xlsx\")    \n",
    "master_bbg_datafile = bbg_datafile.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52886c5-8fc6-4571-be61-942a429d6a1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Ref Levels.xlsx\", index_col=0, parse_dates=True)\n",
    "\n",
    "ref = df[[col for col in df.columns if not col.endswith(\"DUR\")]]\n",
    "\n",
    "dur = df[[col for col in df.columns if col.endswith(\"DUR\")]]\n",
    "dur.loc[datetime.now()] = [np.nan] * len(dur.columns)\n",
    "dur = dur.resample(\"D\").last().ffill().copy()\n",
    "dur = dur.shift().resample(\"1min\").last().ffill().copy()  ############ yesterday's duration we take .. we have shifted it here\n",
    "dur.columns = [item.rsplit(\" \",1)[0] + '_dq_dur' for item in dur.columns]\n",
    "\n",
    "#########################################################################################\n",
    "bbg_tickers = [dict_map[item][8] for item in dict_map.keys()]\n",
    "# reverse_dict = dict(zip(bbg_tickers, list(dict_map.keys())))\n",
    "bbg_data = blp.bdh(tickers = bbg_tickers, flds='px_last', start_date='2017-01-01')\n",
    "bbg_data.columns = bbg_tickers\n",
    "bbg_data.index = pd.to_datetime(bbg_data.index)\n",
    "bbg_data.columns = [reverse_dict[item] for item in bbg_data.columns]\n",
    "\n",
    "for col in ref.columns:\n",
    "    bbg_data[col] = ref[col]\n",
    "\n",
    "bbg_data['ESA'] = bbg_data['SPY']\n",
    "\n",
    "bbg_data1 = bbg_data.resample(\"1min\").last().ffill().copy()\n",
    "bbg_data1.columns = [item +'_bbg_px' for item in bbg_data1.columns]\n",
    "\n",
    "bbg_data2 = bbg_data.shift().resample(\"1min\").last().ffill().copy()\n",
    "bbg_data2.columns = [item +'_bbg_px_2' for item in bbg_data2.columns]\n",
    "\n",
    "################################################## Creating SPY Imputed for recent day in BBG_DATFILE\n",
    "\n",
    "bbg_datafile[\"ESA\"] = bbg_datafile[\"SPY\"]\n",
    "\n",
    "last_close = None\n",
    "for tick in ['ESA INDEX','SPY US EQUITY']:\n",
    "    f = blp.bdib(ticker=tick, flds='PX_LAST', dt=str(list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-2]), ref='IndexUS')\n",
    "    f.index = f.index.tz_convert('America/New_York').tz_localize(None)\n",
    "    f = f.iloc[:,[3]]\n",
    "    f.columns = [tick]\n",
    "    last_close = pd.concat([last_close,f],axis=1)\n",
    "\n",
    "esa_val = last_close['ESA INDEX'].ffill().iloc[-1]\n",
    "spx_val = last_close['SPY US EQUITY'].iloc[-1]\n",
    "\n",
    "tick = \"ESA INDEX\"\n",
    "start_date = list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1]\n",
    "all_f = None\n",
    "for i in range(4):\n",
    "    try:\n",
    "        f = blp.bdib(ticker=tick, flds='PX_LAST', dt=str(start_date+timedelta(days=i)), ref='IndexYieldCurve')\n",
    "        f.index = f.index.tz_convert('America/New_York').tz_localize(None)\n",
    "        f = f.iloc[:,[3]]\n",
    "        f.columns = [tick]\n",
    "        all_f = pd.concat([f, all_f]).sort_index().copy()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "f = all_f.copy()\n",
    "f = f[f.index.date == pd.to_datetime(str(list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1])).date()]\n",
    "f['SPY Imputed'] = round((f/esa_val) * spx_val,2)\n",
    "f = f[['SPY Imputed']].copy()\n",
    "f.columns = ['ESA']\n",
    "\n",
    "recent_spy = bbg_datafile[bbg_datafile.index.date<list(sorted(set(bbg_datafile[['SPY']].dropna().index.date)))[-1]][['ESA']]\n",
    "new_spy = pd.concat([recent_spy, f]).sort_index().copy()\n",
    "bbg_datafile = pd.concat([bbg_datafile.drop(\"ESA\",axis=1), new_spy], axis=1).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691bfc3a-471a-40f5-ae22-e9c4210f28d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = bbg_datafile.copy()\n",
    "er_data = pd.read_csv(\"All ER.csv\",index_col=0, parse_dates=True)\n",
    "\n",
    "er_data.loc[datetime.now().date()] = [np.nan] * len(er_data.columns)\n",
    "er_data.index = pd.to_datetime(er_data.index)\n",
    "er_data = er_data.resample(\"D\").last().ffill().copy()\n",
    "er_data.columns = [item.split(\"ER \",1)[1] for item in er_data.columns]\n",
    "er_data[['ESA']] = er_data[['SPY']]\n",
    "\n",
    "er = er_data.copy()\n",
    "er.columns = [item + '_dq_ER' for item in er.columns]\n",
    "er = er.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "er2 = er_data.shift().copy()\n",
    "er2.columns = [item + '_dq_ER_2' for item in er2.columns]\n",
    "er2 = er2.resample(\"1min\").last().ffill().copy()\n",
    "\n",
    "############################################################ Converting historical price series into historical er series\n",
    "\n",
    "intraday_tr_data = None\n",
    "\n",
    "for col in df.columns:    \n",
    "    first_run = False\n",
    "    if col == \"ESA\":\n",
    "        first_run = True\n",
    "    elif dict_map[col][0] == \"Eq\" or (dict_map[col][0] == \"CDX\" and dict_map[col][4] == 'No'):\n",
    "        first_run = True\n",
    "    \n",
    "    if first_run:\n",
    "        x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']],\\\n",
    "           bbg_data2[[f'{col}_bbg_px_2']],], axis=1).sort_index().dropna().copy()\n",
    "        \n",
    "        x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "        \n",
    "        if col in [\"CDX HY 5Y\", \"CDX HY 10Y\", \"CDX EM 5Y\"]:\n",
    "            x['d-o-d px pnl'] = (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "            x['intraday px pnl'] = (x[col] - x[f'{col}_bbg_px_2']) * 10**(-2)\n",
    "        else:\n",
    "            x['d-o-d px pnl'] = (x[f'{col}_bbg_px']/ x[f'{col}_bbg_px_2'] - 1)\n",
    "            x['intraday px pnl'] = (x[col] / x[f'{col}_bbg_px_2'] - 1)\n",
    "            \n",
    "        x['Calculated TR Change'] = x['TR Change'] - x['d-o-d px pnl'] + x['intraday px pnl']\n",
    "        # x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "        # x = x[['Actual TR Series']].copy()\n",
    "        # x.columns = [col]\n",
    "    \n",
    "    \n",
    "    elif dict_map[col][4] == 'Yes':\n",
    "        x = pd.concat([df[[col]], er[[f'{col}_dq_ER']], er2[[f'{col}_dq_ER_2']], bbg_data1[[f'{col}_bbg_px']], bbg_data2[[f'{col}_bbg_px_2']],\n",
    "            dur[[f'{col}_dq_dur']]], axis=1).sort_index()#.dropna().copy()\n",
    "        x['TR Change'] = (x[f'{col}_dq_ER'] / x[f'{col}_dq_ER_2'] - 1)\n",
    "        x['d-o-d sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[f'{col}_bbg_px'] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "        x['intraday sprd pnl'] = (-1) * (x[f'{col}_dq_dur']) * (x[col] - x[f'{col}_bbg_px_2']) * 10**(-4)\n",
    "        \n",
    "        x['Calculated TR Change'] = x['TR Change'] - x['d-o-d sprd pnl'] + x['intraday sprd pnl']\n",
    "    x['Actual TR Series'] = (1 + x['Calculated TR Change']) * x[f'{col}_dq_ER_2']\n",
    "    x = x[['Actual TR Series']].copy()\n",
    "    x.columns = [col]   \n",
    "    intraday_tr_data = pd.concat([intraday_tr_data, x], axis=1)\n",
    "\n",
    "######################################################################################### Getting ref levels from last close\n",
    "\n",
    "intraday_tr_data1 = intraday_tr_data[intraday_tr_data.index.date<datetime.now().date()].dropna(how='all').copy()\n",
    "last_dict = {}\n",
    "for col in intraday_tr_data1.columns:\n",
    "    last_ref = bbg_datafile.loc[intraday_tr_data1[[col]].dropna().index[-1]][col]\n",
    "    last_date = str(intraday_tr_data1[[col]].dropna().index[-1].date())\n",
    "    last_dict[col] = [intraday_tr_data1[col].dropna().iloc[-1], last_ref, last_date]\n",
    "\n",
    "\n",
    "######################################################################################### calculating today's er series\n",
    "funding = pd.read_excel(\"Funding Rates.xlsx\",index_col=0, parse_dates=True)\n",
    "funding.columns = funding.columns.str.replace(\"GSCBHYEQ\",\"HY Eqty\").str.replace(\"GSCBIGEQ\",\"IG Eqty\")\n",
    "funding['Net Long ESA Funding'] = funding['Net Long SPY Funding']\n",
    "funding['Net Short ESA Funding'] = funding['Net Short SPY Funding']\n",
    "\n",
    "recent_bbg_datafile = bbg_datafile[bbg_datafile.index.date == datetime.now().date()].copy()\n",
    "all_today_er_series = None\n",
    "\n",
    "for col in recent_bbg_datafile.columns:\n",
    "    carry_days = (datetime.now() - pd.to_datetime(last_dict[col][2])).days\n",
    "    x = recent_bbg_datafile[[col]].dropna().copy()\n",
    "    \n",
    "    first_run = False\n",
    "    if col==\"ESA\":\n",
    "        first_run = True\n",
    "    elif dict_map[col][0] == \"Eq\":\n",
    "        first_run = True\n",
    "        \n",
    "    if first_run:\n",
    "        rate = funding[[item for item in funding.columns if col == item.split(\" \",2)[2].rsplit(\" \",1)[0]]].dropna().iloc[-1]\n",
    "        rate = rate.mean()   ############ etf funding rate\n",
    "        x = (last_dict[col][0])*((x/last_dict[col][1])-(rate/100)*(carry_days/360))\n",
    "    elif dict_map[col][4] == \"Yes\":\n",
    "        x = (last_dict[col][0])*(1 - dur[f'{col}_dq_dur'].iloc[-1]*(x-last_dict[col][1])*(10**(-4))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "    elif dict_map[col][0] == \"CDX\":\n",
    "        x = (last_dict[col][0])*(1 + (x-last_dict[col][1])*(10**(-2))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "\n",
    "    all_today_er_series = pd.concat([all_today_er_series, x], axis=1)\n",
    "all_intraday_er_series = pd.concat([intraday_tr_data1, all_today_er_series]).sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d53d2a3-7e40-4dd2-832b-1cbab3ada7af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    # 1 : [\"Intraday\",252,252,'A (Intraday; 12M)'],\n",
    "    # 2 : [\"Intraday\",315,315,'B (Intraday; 15M)'],\n",
    "    3 : [\"Intraday\",378,378,'C (Intraday; 18M)'],\n",
    "}\n",
    "\n",
    "def x_model(model_num, models_list, diff_var, calc_method):\n",
    "    all_zplots = None\n",
    "    val = []\n",
    "    \n",
    "    for global_model in models_list:    \n",
    "        model_Y = global_model[0]\n",
    "        model_X = global_model[1]\n",
    "        zscore_Y = global_model[2]\n",
    "        zscore_X = global_model[3]\n",
    "    \n",
    "        backtest_start_date = pd.to_datetime('2021-12-01')\n",
    "        \n",
    "        zscore_vars = [model_Y, zscore_Y] + model_X + zscore_X\n",
    "        zscore_vars = list(set(zscore_vars))\n",
    "        \n",
    "        if 'ESA' in zscore_vars:\n",
    "            zscore_vars_start_time = '07:30:00'\n",
    "            zscore_vars_end_time = '20:00:00'\n",
    "    \n",
    "        # elif ('ITRX XOVER 5Y' in zscore_vars) and ('CDX HY 5Y' in zscore_vars):\n",
    "        #     zscore_vars_start_time = '07:15:00'\n",
    "        #     zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "    \n",
    "        else:\n",
    "            zscore_vars_start_time = max([dict_map[item][1] for item in zscore_vars])\n",
    "            zscore_vars_end_time = min([dict_map[item][2] for item in zscore_vars])\n",
    "        \n",
    "        ################################## Beta Calculation\n",
    "\n",
    "        # if len(model_X) == 1:\n",
    "        #     er_Y = f'ER {model_Y}'\n",
    "        #     er_X = f'ER {model_X[0]}'\n",
    "        #     er_data = pd.read_csv(\"All ER.csv\")\n",
    "        #     er_data.columns = ['Date'] + list(er_data.columns)[1:]\n",
    "        #     er_data['Date'] = pd.to_datetime(er_data['Date'])\n",
    "        #     er_data = er_data.set_index('Date')\n",
    "        #     er_data = er_data.sort_index()\n",
    "        #     beta = er_data[[er_Y, er_X]].dropna()\n",
    "        #     beta = beta.resample('W').last()\n",
    "        #     beta = np.log(beta)\n",
    "        #     beta = beta.diff().dropna()\n",
    "        #     beta['Beta1'] = [np.nan] * len(beta)\n",
    "        #     beta['Beta2'] = [np.nan] * len(beta)\n",
    "            \n",
    "        #     for i in range(len(beta)-24+1):\n",
    "        #         reg_X = beta[er_X].iloc[i:i+24]\n",
    "        #         reg_Y = beta[er_Y].iloc[i:i+24]\n",
    "        #         model = sm.OLS(reg_Y, sm.add_constant(reg_X)).fit() \n",
    "        #         beta.iloc[i+23,2] = model.params.iloc[1]\n",
    "            \n",
    "        #         model = sm.OLS(reg_X, sm.add_constant(reg_Y)).fit() \n",
    "        #         beta.iloc[i+23,3] = model.params.iloc[1]\n",
    "            \n",
    "        #     beta['Beta1'] = beta['Beta1'].rolling(104).mean()\n",
    "        #     beta['Beta2'] = beta['Beta2'].rolling(104).mean()\n",
    "        #     beta['Beta'] = 0.5*(beta['Beta1'] + 1/ beta['Beta2'])\n",
    "        #     beta = beta[['Beta']].dropna()\n",
    "        # else:\n",
    "        #     ############################################################# Update this beta calculation\n",
    "    \n",
    "        #     b1 = pd.read_csv(\"All Basis Trade Betas.csv\")\n",
    "        #     b1.columns = ['Date'] + list(b1.columns)[1:]\n",
    "        #     b1 = b1.set_index('Date')\n",
    "        #     beta = b1[[f'{model_Y}_{model_X[0]}_{model_X[1]}']]\n",
    "        #     beta.columns = ['Beta']\n",
    "        #     beta['Coef1'] = beta['Beta'].apply(lambda x: eval(x)[0])\n",
    "        #     beta['Coef2'] = beta['Beta'].apply(lambda x: eval(x)[1])\n",
    "        #     beta.index = pd.to_datetime(beta.index)\n",
    "            \n",
    "        # # beta = beta.resample(sampling_freq).first().ffill()\n",
    "        # beta = beta.resample(\"1min\").first().ffill()\n",
    "        \n",
    "        beta = None\n",
    "        \n",
    "        ######################################## Getting data from the master file......\n",
    "        if calc_method == \"Price\":\n",
    "            df = bbg_datafile.copy()    \n",
    "        elif calc_method == \"Return\":\n",
    "            df = all_intraday_er_series.copy()\n",
    "        \n",
    "        zscore_df = df[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).copy()\n",
    "        zscore_df3 = zscore_df.copy()\n",
    "        valid_dates = zscore_df3.index.date\n",
    "        zscore_df3 = zscore_df3.iloc[-6000:].resample(\"1min\").last().ffill().copy()\n",
    "        zscore_df3 = zscore_df3[zscore_vars].between_time(zscore_vars_start_time, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df3 = zscore_df3[pd.Series(zscore_df3.index.date, index=zscore_df3.index).isin(valid_dates)]\n",
    "        zscore_df3 = zscore_df3[zscore_df3.index <= pd.to_datetime(bbg_time)]\n",
    "\n",
    "        start_check = pd.to_datetime(zscore_vars_start_time) - timedelta(minutes=7)\n",
    "        start_check = str(start_check.time())\n",
    "        zscore_df = zscore_df.resample(\"10min\", offset=\"5min\").last().ffill().copy()\n",
    "        zscore_df = zscore_df[zscore_vars].between_time(start_check, zscore_vars_end_time).dropna().copy()\n",
    "        zscore_df = zscore_df[pd.Series(zscore_df.index.date, index=zscore_df.index).isin(valid_dates)]\n",
    "        zscore_df = zscore_df[zscore_df.index >= backtest_start_date]\n",
    "        zscore_df = zscore_df[zscore_df.index <= pd.to_datetime(bbg_time)]\n",
    "        \n",
    "        check_later = zscore_df.copy()\n",
    "        sampling_multiplier = len(set(list(check_later.index.time)))\n",
    "        \n",
    "        \n",
    "        ################################## ZScore Calculation Start : Convert Sprd to PX series\n",
    "        \n",
    "        if calc_method == \"Price\":\n",
    "            zscore_df1 = zscore_df.copy()\n",
    "            df = all_dq_dur.copy()\n",
    "            df.columns = df.columns.str.replace(\" Dur\",\"\")\n",
    "            \n",
    "            last_dq_date = df.index[-1]\n",
    "            last_value = df.iloc[-1,0]\n",
    "            \n",
    "            df.loc[ last_dq_date + timedelta(days=1) ] = last_value\n",
    "            df.loc[ last_dq_date + timedelta(days=2) ] = last_value\n",
    "            \n",
    "            df = df.resample(\"1min\").first().ffill().dropna()\n",
    "            dq_dur = df.copy()\n",
    "            \n",
    "            for col in zscore_df1.columns:\n",
    "                if col in dq_dur.columns:\n",
    "                    zscore_df1[f'{col} Dur'] = dq_dur[col]\n",
    "                    zscore_df1[f'{col} Dur'] = zscore_df1[f'{col} Dur'].shift(1)\n",
    "                    zscore_df1[f'Diff {col}'] = zscore_df1[col].diff()\n",
    "                    zscore_df1 = zscore_df1.dropna()\n",
    "                    zscore_df1[f'{col} Daily PX Change'] = -1 * zscore_df1[f'Diff {col}'] * zscore_df1[f'{col} Dur'] *10**(-4)\n",
    "                    zscore_df1[f'{col} Sum PX'] = zscore_df1[f'{col} Daily PX Change'].cumsum()\n",
    "                    zscore_df1[col] = zscore_df1[f'{col} Sum PX']\n",
    "                    zscore_df1 = zscore_df1[zscore_df.columns].copy()\n",
    "        elif calc_method == \"Return\":\n",
    "            zscore_df1 = zscore_df.copy()\n",
    "        \n",
    "        ################################## ZScore Calculation: Differencing and converting to ZScores\n",
    "        \n",
    "        zscore_df = zscore_df1[zscore_df1.index >= backtest_start_date].copy()\n",
    "        \n",
    "        col_list = zscore_df.columns\n",
    "        for period in diff_var:\n",
    "            for col in col_list:\n",
    "                zscore_df[f'{col}_{period}W'] = zscore_df[col].diff(sampling_multiplier*5*period)\n",
    "        \n",
    "        model_lookback = sampling_multiplier*dict_models[model_num][1]\n",
    "        model_lookback_res = sampling_multiplier*dict_models[model_num][2]\n",
    "        \n",
    "        zscore_df = zscore_df.dropna().copy()\n",
    "        zscore_df2 = zscore_df.copy()\n",
    "    \n",
    "        for period in diff_var:\n",
    "            i = len(zscore_df) - model_lookback\n",
    "            reg_Y = zscore_df[[f'{zscore_Y}_{period}W']].iloc[i:i+model_lookback]\n",
    "            reg_X = zscore_df[[item + f\"_{period}W\" for item in zscore_X]].iloc[i:i+model_lookback]                        \n",
    "            model = sm.OLS(reg_Y,sm.add_constant(reg_X)).fit()\n",
    "            # x = (model.resid - model.resid.rolling(model_lookback_res).mean())/model.resid.rolling(model_lookback_res).std()\n",
    "            x = (model.resid - model.resid.mean())/model.resid.std()\n",
    "            zscore_df.loc[zscore_df.index[i+model_lookback-1],f'{period}W_ZScore'] = x.iloc[-1]\n",
    "        \n",
    "        zscore_df['Avg. ZScore'] = zscore_df[[col for col in zscore_df.columns if col.endswith(\"_ZScore\")]].mean(axis=1)\n",
    "        zscore_df = zscore_df[['Avg. ZScore']]\n",
    "    \n",
    "        bt_df = pd.concat([check_later[[model_Y] + model_X],zscore_df],axis=1).dropna()\n",
    "        bt_df = pd.concat([bt_df,beta],axis=1).dropna()\n",
    "        \n",
    "        zplot = bt_df[['Avg. ZScore']].copy()  \n",
    "        zscore_df3 = zscore_df3.dropna()\n",
    "        val += [zscore_df3.index[-1]]\n",
    "        \n",
    "        if calc_method == \"Price\":\n",
    "            if len(model_X)==1:\n",
    "                zplot.columns = [f'{model_Y}({zscore_df3.iloc[-1][model_Y]}) vs. {model_X[0]}({zscore_df3.\\\n",
    "                iloc[-1][model_X[0]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "            else:\n",
    "                zplot.columns = [f'{model_Y}({zscore_df3.\\\n",
    "                iloc[-1][model_Y]}) vs. {model_X}({zscore_df3.iloc[-1][model_X[0]]} & {zscore_df3.\\\n",
    "                iloc[-1][model_X[1]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "        \n",
    "        elif calc_method == \"Return\":\n",
    "            \n",
    "            px_sprd_ref = bbg_datafile.iloc[-9000:].copy()\n",
    "            px_sprd_ref = px_sprd_ref.resample(\"1min\").last().ffill().sort_index().copy()\n",
    "            \n",
    "            if len(model_X)==1:\n",
    "                zplot.columns = [f'{model_Y}({px_sprd_ref.loc[zscore_df3.index[-1], model_Y]}) vs. {model_X[0]}({px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_X[0]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "            else:\n",
    "                zplot.columns = [f'{model_Y}({px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_Y]}) vs. {model_X}({px_sprd_ref.loc[zscore_df3.index[-1], model_X[0]]} & {px_sprd_ref.\\\n",
    "                    loc[zscore_df3.index[-1], model_X[1]]}) ZScore: {zplot.iloc[-1,0]:.2f}']\n",
    "\n",
    "        all_zplots = pd.concat([all_zplots, zplot],axis=1)\n",
    "        zplot = all_zplots.copy()\n",
    "    return zplot, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc211b8-4e10-4137-bd04-84027f123cea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "items = [\n",
    "    \"CDX IG 5Y\", \"CDX IG 10Y\", \"VCIT\", \"LQD\", \n",
    "    \"CDX HY 5Y\", \"HYG\", \n",
    "    \"SPY\", \"RSP\", \"IJH\", \"IWM\", \"IG Eqty\", \"HY Eqty\", \n",
    "    \"CDX EM 5Y\", \"EMB\", \"EEM\", \n",
    "    \"ITRX MAIN 5Y\", \"ITRX MAIN 10Y\", \"IEAC\", \n",
    "    \"ITRX SNRFIN 5Y\", \"ITRX SUBFIN 5Y\", \n",
    "    \"ITRX XOVER 5Y\", \"IHYG\", \"SX5E\"\n",
    "]\n",
    "\n",
    "pairs = list(combinations(items, 2))\n",
    "\n",
    "mod_list = []\n",
    "for pair in pairs:\n",
    "    mod_list += [[pair[0], [pair[1]]]*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e4771-c543-40df-a1f6-3432d068c63a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(None, display_id=\"DFA\")\n",
    "\n",
    "# while True:\n",
    "# ################################################################## Capture PX; check if market is live and attach\n",
    "# px = blp.bdp(tickers=all_bbg_tickers, flds='PX_LAST').T\n",
    "bbg_time = datetime.now()\n",
    "# px.index= [f'{bbg_time}']\n",
    "# px.index = pd.to_datetime(px.index)\n",
    "# px = px.resample(\"1min\").last()\n",
    "# px.loc[px.index[0],'ESA INDEX'] = round(blp.bdp(tickers=\"ESA INDEX\", flds=\"PX_LAST\").iloc[0,0] * (spx_val/esa_val),2)\n",
    "\n",
    "# if first_current_date_esa_close or bbg_time <= pd.to_datetime(\"09:33:00\"):\n",
    "#     live_list = ['CDX IG CDSI GEN 5Y CORP','ITRX EUR CDSI GEN 5Y CORP', 'RSP US EQUITY','SX5E INDEX','ESA INDEX']\n",
    "#     mkt_live = blp.bdh(tickers=live_list, flds='px_last', start_date = datetime.now()-timedelta(days=5))\n",
    "#     mkt_live.columns = live_list\n",
    "#     first_current_date_esa_close = False\n",
    "\n",
    "# for col in px.columns:\n",
    "#     if col == \"ESA INDEX\" and bbg_time.date() in mkt_live[live_dict[col]].dropna().index:\n",
    "#         if bbg_time.time() >= pd.to_datetime(\"07:30\").time() and\\\n",
    "#         bbg_time.time() <= (pd.to_datetime(\"20:00\") + timedelta(minutes=1)).time():\n",
    "#             continue\n",
    "#         else:\n",
    "#             px.loc[px.index[0],col] = np.nan\n",
    "#     elif bbg_time.date() in mkt_live[live_dict[col]].dropna().index and\\\n",
    "#     bbg_time.time() >= pd.to_datetime(dict_map[reverse_dict[col]][1]).time() and\\\n",
    "#     bbg_time.time() <= (pd.to_datetime(dict_map[reverse_dict[col]][2]) + timedelta(minutes=1)).time():\n",
    "#         continue\n",
    "#     else:\n",
    "#         px.loc[px.index[0],col] = np.nan\n",
    "# px.columns = [reverse_dict[item] for item in px.columns]\n",
    "# bbg_datafile = pd.concat([bbg_datafile, px]).sort_index().copy()\n",
    "\n",
    "# bbg_datafile = bbg_datafile.resample(\"1min\").last().dropna(how='all').copy()\n",
    "\n",
    "################################################################## Convert PX to ER series and attach\n",
    "# current_er_series = None\n",
    "\n",
    "# for col in px.columns:\n",
    "#     carry_days = (datetime.now() - pd.to_datetime(last_dict[col][2])).days\n",
    "#     x = px[[col]].dropna().copy()\n",
    "\n",
    "#     first_run = False\n",
    "#     if col==\"ESA\":\n",
    "#         first_run = True\n",
    "#     elif dict_map[col][0] == \"Eq\":\n",
    "#         first_run = True\n",
    "        \n",
    "#     if first_run:\n",
    "#         rate = funding[[item for item in funding.columns if col == item.split(\" \",2)[2].rsplit(\" \",1)[0]]].dropna().iloc[-1]\n",
    "#         rate = rate.mean()   ############ etf funding rate\n",
    "#         x = (last_dict[col][0])*((x/last_dict[col][1])-(rate/100)*(carry_days/360))\n",
    "#     elif dict_map[col][4] == \"Yes\":\n",
    "#         x = (last_dict[col][0])*(1 - dur[f'{col}_dq_dur'].iloc[-1]*(x-last_dict[col][1])*(10**(-4))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "#     elif dict_map[col][0] == \"CDX\":\n",
    "#         x = (last_dict[col][0])*(1 + (x-last_dict[col][1])*(10**(-2))+(dict_map[col][3]/100)*(carry_days/360))\n",
    "    \n",
    "#     current_er_series = pd.concat([current_er_series, x], axis=1)\n",
    "# all_intraday_er_series = pd.concat([all_intraday_er_series, current_er_series]).sort_index().copy()\n",
    "# all_intraday_er_series = all_intraday_er_series.resample(\"1min\").last().dropna(how='all').copy()\n",
    "\n",
    "last_update_time = all_intraday_er_series.copy()\n",
    "\n",
    "##################################################################  Regressions starts from here\n",
    "all_zscore_values = []\n",
    "\n",
    "for diff_list in [[1,2,3],[6,9,12]]:        \n",
    "    times_list = []\n",
    "\n",
    "    if diff_list == [1,2,3]:\n",
    "        calc = \"Price\"\n",
    "    elif diff_list == [6,9,12]:\n",
    "        calc = \"Return\"            \n",
    "    \n",
    "    # model_Y, model_X (specify as a list) ### We trade these\n",
    "    # zscore_Y, zscore_X (specify as a list) ### We use these only for generating the zscore; names are taken from BBG datafile\n",
    "    \n",
    "    models_list = [\n",
    "        ['CDX IG 5Y', ['VCIT','IEF'], 'CDX IG 5Y', ['VCIT','IEF'],],\n",
    "        ['CDX HY 5Y', ['HYG','IEI'], 'CDX HY 5Y', ['HYG','IEI'],],\n",
    "        ['CDX EM 5Y', ['EMB','IEF'], 'CDX EM 5Y', ['EMB','IEF'],],\n",
    "    ]\n",
    "    \n",
    "    l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "    l2 = l1.copy()\n",
    "    times_list += t1\n",
    "    \n",
    "    models_list = mod_list    \n",
    "    l1, t1 = x_model(3, models_list, diff_list, calc)\n",
    "    l2 = pd.concat([l2,l1],axis=1)\n",
    "    times_list += t1  \n",
    "\n",
    "    all_zscore_values += [[item for item in l2.columns]]\n",
    "\n",
    "# ########################################################################################## Combining them\n",
    "\n",
    "df = pd.DataFrame({'PX_Zscore 1/2/3w':all_zscore_values[0]})\n",
    "df['Rtn_Zscore 6/9/12w'] = [item.split(\": \")[1] for item in all_zscore_values[1]]\n",
    "\n",
    "df['Pair'] = df['PX_Zscore 1/2/3w'].apply(lambda x: x.split(\" ZScore: \")[0])\n",
    "df['PX_Zscore 1/2/3w'] = df['PX_Zscore 1/2/3w'].apply(lambda x: x.split(\" ZScore: \")[1])\n",
    "df['Pair'] = df['Pair'].str.replace(\"(\",\" (\")\n",
    "df['Long Risk'] = df['Pair'].apply(lambda x: x.split(\" vs. \")[0])\n",
    "df['Short Risk'] = df['Pair'].apply(lambda x: x.split(\" vs. \")[1])\n",
    "df = df[['Long Risk','Short Risk','PX_Zscore 1/2/3w','Rtn_Zscore 6/9/12w']]\n",
    "df['Long Ref.'] = df['Long Risk'].apply(lambda x: x.split(\" (\")[1].replace(\")\",\"\"))\n",
    "df['Long Risk'] = df['Long Risk'].apply(lambda x: x.split(\" (\")[0].replace(\"CDX \",\"\").\\\n",
    "                                        replace(\"ITRX \",\"\").replace(\" 5Y\",\"\").replace(\"SPX\",\"ESA Implied SPY\"))\n",
    "df['Short Ref.'] = df['Short Risk'].apply(lambda x: x.split(\" (\")[1].replace(\")\",\"\"))\n",
    "df['Short Ref.'] = df['Short Ref.'].apply(lambda x: f\"['{x.split(\" & \")[0]}', '{x.split(\" & \")[1]}']\" if \"&\" in x else x)\n",
    "df['Short Risk'] = df['Short Risk'].apply(lambda x: x.split(\" (\")[0])\n",
    "df['Short Risk'] = df['Short Risk'].apply(lambda x: x.replace(\"ITRX \",\"\").\\\n",
    "                                          replace(\" 5Y\",\"\").replace(\"SPX\",\"ESA Implied SPY\").replace(\"CDX \",\"\"))\n",
    "df['Valuation'] = df.apply(lambda row: f\"{row['Long Risk']} {'rich' if eval(row['PX_Zscore 1/2/3w'])\\\n",
    ">0 else 'cheap'} to {row['Short Risk']}\",axis=1)\n",
    "\n",
    "df = df[['Long Risk','Long Ref.','Short Risk','Short Ref.','PX_Zscore 1/2/3w','Rtn_Zscore 6/9/12w','Valuation']].copy()\n",
    "df['Last Update'] = [item.strftime(\"%d-%b %I:%M %p\") for item in times_list]\n",
    "df = df.rename(columns={\"Long Ref.\":\"Ref\",\"Short Ref.\":\"Ref.\"})\n",
    "\n",
    "formatted_time = bbg_time.strftime(\"%I:%M %p\")\n",
    "df.columns = pd.MultiIndex.from_tuples([(formatted_time, col) for col in df.columns])\n",
    "\n",
    "styled_df = (\n",
    "    df.style\n",
    "    .apply(color_negative_red_positive_green_basis, subset=[df.columns[4]], axis=0)\n",
    "    .apply(color_negative_red_positive_green_basis, subset=[df.columns[5]], axis=0)\n",
    "    .applymap(bold_zscore, subset=pd.IndexSlice[:, df.columns[4]])\n",
    "    .apply(add_black_line, axis=1)\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "update_display(styled_df,display_id=\"DFA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edc8cc-9626-461f-bd5b-e27451d2d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfx = df.copy()\n",
    "# dfx.columns = [item[1] for item in dfx.columns]\n",
    "# dfx[['PX_Zscore 1/2/3w','Rtn_Zscore 6/9/12w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8baaf-d8f3-4450-a53c-e32b8747463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_excel(\"Data for Credit-Eqty Dashboard v3.xlsx\",index_col=0,parse_dates=True)\n",
    "# b = pd.read_excel(\"New Items Data.xlsx\",index_col=0,parse_dates=True)\n",
    "# c = pd.concat([a,b],axis=1).sort_index()\n",
    "# c.to_excel(\"Data for Credit-Eqty Dashboard v4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c098d35-a42d-4b27-afed-57d5bccb2084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f65f2b-3622-4aa3-80d8-3649630ad0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
